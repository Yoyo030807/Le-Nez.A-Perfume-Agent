import os
import json
from datetime import datetime
from typing import Literal, Optional, List
import re
import requests
import time
import random

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, BackgroundTasks  # type: ignore
from fastapi.middleware.cors import CORSMiddleware  # type: ignore
from fastapi.responses import StreamingResponse  # type: ignore
from openai import OpenAI  # type: ignore
from pydantic import BaseModel  # type: ignore
import httpx

# å°è¯•å¯¼å…¥tavilyï¼ˆè”ç½‘æœç´¢ï¼‰
TAVILY_AVAILABLE = False
TavilyClient = None
try:
    from tavily import TavilyClient  # type: ignore
    TAVILY_AVAILABLE = True
except ImportError:
    pass

# å°è¯•å¯¼å…¥ duckduckgo-search
DDGS_AVAILABLE = False
try:
    from duckduckgo_search import DDGS  # type: ignore
    DDGS_AVAILABLE = True
except ImportError:
    pass


# åŠ è½½ .env ç¯å¢ƒå˜é‡
load_dotenv()

# ä»ç¯å¢ƒå˜é‡è¯»å– OpenAIï¼ˆç™¾åº¦åƒå¸†å…¼å®¹ï¼‰é…ç½®
OPENAI_API_KEY: Optional[str] = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL: Optional[str] = os.getenv("OPENAI_BASE_URL")
LLM_MODEL_ID: Optional[str] = os.getenv("LLM_MODEL_ID", "deepseek-v3.2")  # é»˜è®¤ä½¿ç”¨ deepseek-v3.2

if not OPENAI_API_KEY or not OPENAI_BASE_URL:
    # æå‰æš´éœ²é…ç½®é—®é¢˜ï¼Œé¿å…è¿è¡Œæ—¶æ‚„æ‚„å¤±è´¥
    raise RuntimeError(
        "ç¯å¢ƒå˜é‡ OPENAI_API_KEY æˆ– OPENAI_BASE_URL æœªé…ç½®ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚"
    )

print(f"[é…ç½®] LLM_MODEL_ID: {LLM_MODEL_ID}")
print(f"[é…ç½®] OPENAI_BASE_URL: {OPENAI_BASE_URL}")

client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL,
)

# åˆå§‹åŒ–Tavilyå®¢æˆ·ç«¯ï¼ˆè”ç½‘æœç´¢ï¼‰
TAVILY_API_KEY: Optional[str] = os.getenv("TAVILY_API_KEY")
tavily_client = None

print(f"[Tavilyåˆå§‹åŒ–] TAVILY_AVAILABLE: {TAVILY_AVAILABLE}")
print(f"[Tavilyåˆå§‹åŒ–] TAVILY_API_KEY å­˜åœ¨: {TAVILY_API_KEY is not None}")
if TAVILY_API_KEY:
    print(f"[Tavilyåˆå§‹åŒ–] TAVILY_API_KEY é•¿åº¦: {len(TAVILY_API_KEY)}")
    print(f"[Tavilyåˆå§‹åŒ–] TAVILY_API_KEY å‰ç¼€: {TAVILY_API_KEY[:10]}...")

if TAVILY_AVAILABLE and TAVILY_API_KEY:
    try:
        tavily_client = TavilyClient(api_key=TAVILY_API_KEY)
        print(f"[Tavilyåˆå§‹åŒ–] âœ… Tavilyå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"[Tavilyåˆå§‹åŒ–] âŒ Tavilyåˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        tavily_client = None
else:
    if not TAVILY_AVAILABLE:
        print(f"[Tavilyåˆå§‹åŒ–] âš ï¸ Tavilyåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install tavily-python")
    if not TAVILY_API_KEY:
        print(f"[Tavilyåˆå§‹åŒ–] âš ï¸ TAVILY_API_KEY æœªåœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")


class ChatMessage(BaseModel):
    role: Literal["user", "assistant"]
    content: str


class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    locale: Literal["zh", "en"] = "zh"
    conversation_id: Optional[str] = None
    user_name: Optional[str] = None


app = FastAPI(title="Scent Alchemist Chat API")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CONVERSATIONS_DIR = os.path.join(BASE_DIR, "conversations")
RECIPES_DIR = os.path.join(BASE_DIR, "recipes")
os.makedirs(CONVERSATIONS_DIR, exist_ok=True)
os.makedirs(RECIPES_DIR, exist_ok=True)


def _conversation_path(conversation_id: str) -> str:
    return os.path.join(CONVERSATIONS_DIR, f"{conversation_id}.json")


def _recipe_path(recipe_id: str) -> str:
    return os.path.join(RECIPES_DIR, f"{recipe_id}.json")


def _load_recipe(recipe_id: str) -> dict:
    path = _recipe_path(recipe_id)
    if not os.path.exists(path):
        raise HTTPException(status_code=404, detail="Recipe not found")
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def _save_recipe(recipe_id: str, data: dict) -> None:
    path = _recipe_path(recipe_id)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def _save_conversation(conversation_id: str, messages: List[dict], user_name: Optional[str] = None) -> None:
    path = _conversation_path(conversation_id)
    now = datetime.utcnow().isoformat() + "Z"
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception:
            data = {}
        created_at = data.get("created_at", now)
        # ä¿ç•™å·²æœ‰çš„ç”¨æˆ·åå­—ï¼Œé™¤éä¼ å…¥æ–°çš„
        if user_name is None:
            user_name = data.get("user_name")
    else:
        created_at = now

    payload = {
        "id": conversation_id,
        "created_at": created_at,
        "updated_at": now,
        "messages": messages,
        "user_name": user_name,
        "last_message_time": now,  # è®°å½•æœ€åä¸€æ¡æ¶ˆæ¯çš„æ—¶é—´
    }
    # ä¿ç•™å·²æœ‰çš„æ‰‹æœ­å’Œä¸Šæ¬¡ç”Ÿæˆæ‰‹æœ­æ—¶çš„æ¶ˆæ¯æ•°é‡
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                old_data = json.load(f)
            payload["memo"] = old_data.get("memo")
            payload["memo_last_message_count"] = old_data.get("memo_last_message_count", 0)
            payload["last_memo_time"] = old_data.get("last_memo_time")  # ä¿ç•™ä¸Šæ¬¡ç”Ÿæˆæ‰‹æœ­çš„æ—¶é—´
        except Exception:
            pass
    else:
        # æ–°ä¼šè¯ï¼Œåˆå§‹åŒ–æ‰‹æœ­ç›¸å…³å­—æ®µ
        payload["memo_last_message_count"] = 0
    
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)


def _load_conversation(conversation_id: str) -> dict:
    path = _conversation_path(conversation_id)
    if not os.path.exists(path):
        raise HTTPException(status_code=404, detail="Conversation not found")
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def _extract_user_name(messages: List[dict], existing_name: Optional[str] = None) -> Optional[str]:
    """ä»å¯¹è¯ä¸­æå–ç”¨æˆ·åå­—"""
    if existing_name:
        return existing_name
    
    # æŸ¥æ‰¾åŠ©æ‰‹è¯¢é—®åå­—åçš„ç”¨æˆ·å›å¤
    for i, msg in enumerate(messages):
        if msg.get("role") == "assistant":
            content = msg.get("content", "").lower()
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯¢é—®åå­—çš„æç¤º
            if any(keyword in content for keyword in ["ç§°å‘¼", "åå­—", "name", "address", "call"]):
                # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯
                for j in range(i + 1, len(messages)):
                    if messages[j].get("role") == "user":
                        user_content = messages[j].get("content", "").strip()
                        # ç®€å•æå–ï¼šå–å‰10ä¸ªå­—ç¬¦ï¼Œå»é™¤æ ‡ç‚¹
                        name = re.sub(r'[^\w\s\u4e00-\u9fff]', '', user_content[:10]).strip()
                        if name and len(name) <= 20:  # åˆç†é•¿åº¦
                            return name
    return None


def _get_solar_term(year: int, month: int, day: int) -> str:
    """è®¡ç®—äºŒåå››èŠ‚æ°”"""
    # äºŒåå››èŠ‚æ°”æ—¥æœŸè¡¨ï¼ˆç®€åŒ–ç‰ˆï¼ŒåŸºäº2024-2025å¹´ï¼‰
    # å®é™…åº”è¯¥ä½¿ç”¨ç²¾ç¡®çš„å¤©æ–‡è®¡ç®—ï¼Œè¿™é‡Œä½¿ç”¨è¿‘ä¼¼å€¼
    solar_terms = [
        (1, 5, "å°å¯’"), (1, 20, "å¤§å¯’"), (2, 4, "ç«‹æ˜¥"), (2, 19, "é›¨æ°´"),
        (3, 5, "æƒŠè›°"), (3, 20, "æ˜¥åˆ†"), (4, 5, "æ¸…æ˜"), (4, 20, "è°·é›¨"),
        (5, 5, "ç«‹å¤"), (5, 21, "å°æ»¡"), (6, 6, "èŠ’ç§"), (6, 21, "å¤è‡³"),
        (7, 7, "å°æš‘"), (7, 23, "å¤§æš‘"), (8, 7, "ç«‹ç§‹"), (8, 23, "å¤„æš‘"),
        (9, 8, "ç™½éœ²"), (9, 23, "ç§‹åˆ†"), (10, 8, "å¯’éœ²"), (10, 23, "éœœé™"),
        (11, 7, "ç«‹å†¬"), (11, 22, "å°é›ª"), (12, 7, "å¤§é›ª"), (12, 22, "å†¬è‡³"),
    ]
    
    # æ‰¾åˆ°æœ€æ¥è¿‘çš„èŠ‚æ°”
    for m, d, term in solar_terms:
        if month == m:
            if day >= d:
                return term
            # å¦‚æœè¿˜æ²¡åˆ°è¿™ä¸ªæœˆçš„èŠ‚æ°”ï¼Œè¿”å›ä¸Šä¸ªæœˆçš„æœ€åä¸€ä¸ªèŠ‚æ°”
            prev_month = m - 1 if m > 1 else 12
            for pm, pd, pterm in reversed(solar_terms):
                if pm == prev_month:
                    return pterm
    # é»˜è®¤è¿”å›å†¬è‡³ï¼ˆ12æœˆ22æ—¥ä¹‹åï¼‰
    return "å†¬è‡³"


async def _get_location_and_weather() -> dict:
    """è·å–ç”¨æˆ·ä½ç½®å’Œå¤©æ°”ä¿¡æ¯ï¼ˆåŒ…æ‹¬æ¸©åº¦ï¼‰"""
    try:
        # ä½¿ç”¨å…è´¹çš„IPå®šä½æœåŠ¡
        async with httpx.AsyncClient(timeout=5.0) as client:
            # è·å–IPå’Œä½ç½®
            ip_response = await client.get("https://ipapi.co/json/")
            if ip_response.status_code == 200:
                ip_data = ip_response.json()
                lat = ip_data.get("latitude")
                lon = ip_data.get("longitude")
                city = ip_data.get("city", "Unknown")
                country = ip_data.get("country_name", "Unknown")
                timezone = ip_data.get("timezone", "UTC")
                
                temperature = None
                # å°è¯•è·å–æ¸©åº¦ï¼ˆä½¿ç”¨å…è´¹çš„å¤©æ°”APIï¼‰
                if lat and lon:
                    try:
                        # ä½¿ç”¨wttr.inå…è´¹å¤©æ°”API
                        weather_url = f"https://wttr.in/?format=j1"
                        weather_response = await client.get(weather_url, timeout=3.0)
                        if weather_response.status_code == 200:
                            weather_data = weather_response.json()
                            if "current_condition" in weather_data:
                                temp_c = weather_data["current_condition"][0].get("temp_C")
                                if temp_c:
                                    temperature = int(float(temp_c))
                    except Exception:
                        pass
                
                weather_info = {
                    "location": f"{city}, {country}",
                    "timezone": timezone,
                    "temperature": temperature,
                    "coordinates": {"lat": lat, "lon": lon} if lat and lon else None,
                }
                
                return weather_info
    except Exception:
        pass
    
    return {"location": "Unknown", "timezone": "UTC", "temperature": None}


async def _generate_memo_summary(conversation_data: dict, locale: str = "zh", is_update: bool = False) -> str:
    """ç”ŸæˆLe Nezå…ˆç”Ÿçš„æ‰‹æœ­æ ¼å¼æ‘˜è¦"""
    messages = conversation_data.get("messages", [])
    user_name = conversation_data.get("user_name", "æœ‹å‹")
    created_at = conversation_data.get("created_at", datetime.utcnow().isoformat())
    
    # è§£ææ—¥æœŸ
    try:
        dt = datetime.fromisoformat(created_at.replace("Z", "+00:00"))
    except:
        dt = datetime.utcnow()
    
    # è·å–ä½ç½®å’Œå¤©æ°”
    location_data = await _get_location_and_weather()
    location = location_data.get("location", "æœªçŸ¥åœ°ç‚¹")
    timezone = location_data.get("timezone", "UTC")
    temperature = location_data.get("temperature")
    
    # æ ¼å¼åŒ–æ—¥æœŸï¼ˆæ ¹æ®æ—¶åŒºï¼‰
    try:
        import pytz
        tz = pytz.timezone(timezone)
        local_dt = dt.astimezone(tz)
    except:
        local_dt = dt
    
    # è·å–äºŒåå››èŠ‚æ°”
    solar_term = _get_solar_term(local_dt.year, local_dt.month, local_dt.day)
    
    date_str = local_dt.strftime("%Yå¹´%mæœˆ%dæ—¥" if locale == "zh" else "%B %d, %Y")
    time_str = local_dt.strftime("%H:%M")
    
    # æ„å»ºå¤©æ°”å’ŒèŠ‚æ°”ä¿¡æ¯
    weather_info = ""
    if locale == "zh":
        if temperature is not None:
            weather_info = f"ï¼Œæ°”æ¸©{temperature}Â°C"
        weather_info += f"ï¼Œ{solar_term}"
    else:
        if temperature is not None:
            weather_info = f", {temperature}Â°C"
        weather_info += f", {solar_term}"
    
    # æå–å¯¹è¯å…³é”®ä¿¡æ¯
    user_messages = [m.get("content", "") for m in messages if m.get("role") == "user"]
    assistant_messages = [m.get("content", "") for m in messages if m.get("role") == "assistant"]
    
    # ä½¿ç”¨LLMç”Ÿæˆè¯—æ„æ‘˜è¦
    if is_update:
        # è¿½åŠ æ›´æ–°çš„æç¤ºè¯
        summary_prompt = f"""ä½ æ˜¯ä¸€ä½æ³•å›½è°ƒé¦™å¸ˆ Le Nezï¼Œæ­£åœ¨æ‰‹æœ­ä¸­è¿½åŠ è®°å½•ä¸{user_name}çš„åç»­å¯¹è¯ã€‚

è¿™æ¬¡å¯¹è¯å‘ç”Ÿåœ¨{date_str} {time_str}{weather_info}ï¼Œåœ°ç‚¹ï¼š{location}

æ–°å¢å¯¹è¯çš„ä¸»è¦å†…å®¹ï¼š
{chr(10).join(user_messages[:3])}

è¯·ç”¨è¯—æ„ã€ç®€æ´çš„è¯­è¨€ï¼Œä»¥æ‰‹æœ­/å¤‡å¿˜å½•çš„å½¢å¼ï¼Œè®°å½•è¿™æ¬¡åç»­å¯¹è¯ã€‚åŒ…æ‹¬ï¼š
1. æ–°çš„æ—¥æœŸã€æ—¶é—´ã€æ¸©åº¦ã€èŠ‚æ°”å’Œå¤©æ°”ï¼ˆå¦‚æœä¸ä¹‹å‰ä¸åŒï¼‰
2. {user_name}çš„æ–°å¿ƒæƒ…å’ŒçŠ¶æ€å˜åŒ–
3. æ–°çš„é¦™æ°›è®¨è®ºæˆ–åå¥½å˜åŒ–
4. å¯¹{user_name}çš„æ–°è§‚å¯Ÿæˆ–å»ºè®®

ç”¨ç¬¬ä¸€äººç§°ï¼Œåƒåœ¨å†™ç§äººç¬”è®°çš„ç»­ç¯‡ã€‚è¯­è¨€è¦ä¼˜é›…ã€ç®€æ´ï¼Œä¸è¶…è¿‡100å­—ã€‚å¯ä»¥è‡ªç„¶åœ°æ‰¿æ¥ä¹‹å‰çš„è®°å½•ã€‚

é‡è¦ï¼šä¸è¦åœ¨æ‰‹æœ­æœ«å°¾æ·»åŠ å­—æ•°ç»Ÿè®¡ã€‚åªè¾“å‡ºæ‰‹æœ­å†…å®¹æœ¬èº«ã€‚

{"ç”¨ä¸­æ–‡" if locale == "zh" else "Use English"}"""
    else:
        # é¦–æ¬¡ç”Ÿæˆçš„æç¤ºè¯
        summary_prompt = f"""ä½ æ˜¯ä¸€ä½æ³•å›½è°ƒé¦™å¸ˆ Le Nezï¼Œæ­£åœ¨å†™æ‰‹æœ­è®°å½•ä¸€æ¬¡ä¸{user_name}çš„å¯¹è¯ã€‚

å¯¹è¯å‘ç”Ÿåœ¨{date_str} {time_str}{weather_info}ï¼Œåœ°ç‚¹ï¼š{location}

ç”¨æˆ·çš„ä¸»è¦ä¿¡æ¯ï¼š
{chr(10).join(user_messages[:3])}

è¯·ç”¨è¯—æ„ã€ç®€æ´çš„è¯­è¨€ï¼Œä»¥æ‰‹æœ­/å¤‡å¿˜å½•çš„å½¢å¼ï¼Œæ€»ç»“è¿™æ¬¡å¯¹è¯ã€‚åŒ…æ‹¬ï¼š
1. æ—¥æœŸã€æ—¶é—´ã€æ¸©åº¦ã€èŠ‚æ°”å’Œå¤©æ°”ï¼ˆå¯ä»¥è¯—æ„æè¿°ï¼‰
2. åœ°ç‚¹
3. {user_name}çš„å¿ƒæƒ…å’ŒçŠ¶æ€
4. é€‰æ‹©çš„é¦™æ°›æˆ–åå¥½
5. å¯¹{user_name}çš„é¼“åŠ±æˆ–å»ºè®®

ç”¨ç¬¬ä¸€äººç§°ï¼Œåƒåœ¨å†™ç§äººç¬”è®°ä¸€æ ·ã€‚è¯­è¨€è¦ä¼˜é›…ã€ç®€æ´ï¼Œä¸è¶…è¿‡150å­—ã€‚

é‡è¦ï¼šä¸è¦åœ¨æ‰‹æœ­æœ«å°¾æ·»åŠ å­—æ•°ç»Ÿè®¡ã€‚åªè¾“å‡ºæ‰‹æœ­å†…å®¹æœ¬èº«ã€‚

{"ç”¨ä¸­æ–‡" if locale == "zh" else "Use English"}"""

    try:
        completion = client.chat.completions.create(
            model="deepseek-v3.2",
            messages=[
                {"role": "system", "content": "You are Le Nez, a French perfumer writing personal notes. Write in a poetic, concise style."},
                {"role": "user", "content": summary_prompt}
            ],
            temperature=0.8,
            max_tokens=300,
        )
        summary = completion.choices[0].message.content.strip()
        
        # ç§»é™¤LLMå¯èƒ½æ·»åŠ çš„å­—æ•°ç»Ÿè®¡ï¼ˆå¦‚"å­—æ•°: 98"ã€"å­—æ•¸: 98"ç­‰ï¼‰
        import re
        if locale == "zh":
            # ç§»é™¤ä¸­æ–‡å­—æ•°ç»Ÿè®¡
            summary = re.sub(r'\n?\s*å­—[æ•¸æ•°]:\s*\d+\s*$', '', summary, flags=re.MULTILINE)
            summary = re.sub(r'\n?\s*å­—æ•°:\s*\d+\s*$', '', summary, flags=re.MULTILINE)
        else:
            # ç§»é™¤è‹±æ–‡å­—æ•°ç»Ÿè®¡
            summary = re.sub(r'\n?\s*[Ww]ord\s*[Cc]ount:?\s*\d+\s*$', '', summary, flags=re.MULTILINE)
        
        return summary.strip()
    except Exception as e:
        # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œè¿”å›ç®€å•æ ¼å¼
        if locale == "zh":
            return f"""{date_str} {time_str}
{location}

ä»Šæ—¥ä¸{user_name}çš„å¯¹è¯ã€‚{user_messages[0][:50] if user_messages else "..."}

è®°å½•äºæ‰‹æœ­ã€‚"""
        else:
            return f"""{date_str} {time_str}
{location}

Conversation with {user_name}. {user_messages[0][:50] if user_messages else "..."}

Noted in journal."""

# CORS è®¾ç½®ï¼šå‰ç«¯æœ¬åœ°å¼€å‘ä½¿ç”¨ 5174 ç«¯å£
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5174",
        "http://127.0.0.1:5174",
        "http://localhost:5173",  # ä¿ç•™æ—§ç«¯å£å…¼å®¹
        "http://127.0.0.1:5173",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


async def detect_intent(messages: List[dict]) -> bool:
    """
    LLM æ„å›¾è¯†åˆ«ï¼šåˆ¤æ–­ç”¨æˆ·æ˜¯å¦éœ€è¦å¤–éƒ¨çŸ¥è¯†ï¼ˆå®æ—¶æ•°æ®ã€æ­Œè¯ã€æ–°é—»ã€äº‹å®ç­‰ï¼‰
    
    æ¥æ”¶å®Œæ•´çš„å¯¹è¯å†å² messagesï¼Œä»¥ä¾¿æ›´å‡†ç¡®åˆ¤æ–­æ„å›¾
    
    è¿”å› True è¡¨ç¤ºéœ€è¦æœç´¢ï¼ŒFalse è¡¨ç¤ºä¸éœ€è¦
    """
    # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    last_user_message = ""
    for msg in reversed(messages):
        if msg.get("role") == "user":
            last_user_message = msg.get("content", "")
            break
    
    if not last_user_message:
        return False
    
    # å…³é”®è¯å¿«é€Ÿé€šé“ï¼šå‡å°‘ LLM æ¶ˆè€—ï¼ˆæ‰©å±•å…³é”®è¯åˆ—è¡¨ï¼‰
    quick_search_keywords = [
        "æœç´¢", "æŸ¥ä¸€ä¸‹", "æŸ¥æ‰¾", "å¸®æˆ‘æŸ¥", "èƒ½å¦æœç´¢", "search", "lookup", "find",
        "æ­Œè¯", "lyrics", "æ˜¯è°", "å“ªä¸€å¹´", "ä»€ä¹ˆæ—¶å€™", "where", "when", "who",
        "ä½ çŸ¥é“", "çŸ¥é“", "äº†è§£", "ä½ äº†è§£", "ä½ å¬è¯´è¿‡", "å¬è¯´è¿‡",
        "è‹±æ–‡å", "å…¨å", "å«ä»€ä¹ˆ", "å“ªé‡Œä¹°", "ä»·æ ¼", "å¤šå°‘é’±", "how much", "price", "buy", "where to buy"
    ]
    user_msg_lower = last_user_message.lower()
    
    # å¦‚æœåŒ…å«æ˜æ˜¾çš„å…³é”®è¯ï¼Œç›´æ¥è¿”å› Trueï¼Œè·³è¿‡ LLM åˆ¤æ–­
    if any(kw in user_msg_lower for kw in quick_search_keywords):
        print(f"[æ„å›¾è¯†åˆ«] å¿«é€Ÿé€šé“ï¼šæ£€æµ‹åˆ°æœç´¢å…³é”®è¯ï¼Œç›´æ¥è¿”å› True")
        return True
    
    # ä½¿ç”¨ LLM è¿›è¡Œæ„å›¾è¯†åˆ«ï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡ï¼‰
    try:
        # æ„å»ºä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆæœ€è¿‘3æ¡æ¶ˆæ¯ï¼‰
        context_summary = ""
        recent_messages = messages[-3:] if len(messages) > 3 else messages
        for msg in recent_messages:
            role = msg.get("role", "")
            content = msg.get("content", "")[:100]  # é™åˆ¶é•¿åº¦
            if role == "user":
                context_summary += f"User: {content}\n"
            elif role == "assistant":
                context_summary += f"Bot: {content}\n"
        
        intent_prompt = f"""You are an Intent Classifier. Analyze the user's latest message and conversation context.
Does the user need EXTERNAL KNOWLEDGE (real-time data, specific lyrics, news, facts, celebrity info) to get a good answer?

Conversation Context:
{context_summary}

Latest User Message: {last_user_message}

Examples:
- "Hi" -> NO
- "I am sad" -> NO
- "Who won the game yesterday?" -> YES
- "Lyrics of 'Yesterday'" -> YES
- "Recommend a perfume" -> NO (Le Nez can handle this internally)
- "Analyze this perfume: Oud Wood" -> YES (Needs factual data)
- "èŠèŠã€Šé©¾é¹¤è¥¿å»ã€‹" -> YES (Needs lyrics or song info)
- "What is Chanel No. 5?" -> YES (Needs factual perfume data)
- "å®ƒçš„è‹±æ–‡åå«ä»€ä¹ˆ" -> YES (Needs to resolve pronoun from context)

Return ONLY the word "YES" or "NO"."""
        
        response = client.chat.completions.create(
            model=LLM_MODEL_ID,
            messages=[
                {"role": "system", "content": "You are an Intent Classifier. Return only 'YES' or 'NO'."},
                {"role": "user", "content": intent_prompt}
            ],
            temperature=0,  # ç¡®ä¿ç¨³å®š
            max_tokens=10
        )
        
        result = response.choices[0].message.content.strip().upper()
        should_search = result == "YES"
        
        print(f"[æ„å›¾è¯†åˆ«] LLM åˆ¤æ–­ç»“æœ: {result} -> should_search: {should_search}")
        return should_search
        
    except Exception as e:
        error_msg = str(e)
        print(f"[æ„å›¾è¯†åˆ«] LLM è°ƒç”¨å¤±è´¥: {error_msg}")
        if "401" in error_msg or "Unauthorized" in error_msg:
            print(f"[æ„å›¾è¯†åˆ«] âŒ 401 é”™è¯¯ï¼šè¯·æ£€æŸ¥ .env ä¸­çš„ LLM_MODEL_ID æ˜¯å¦æ­£ç¡®ï¼Œå½“å‰å€¼: {LLM_MODEL_ID}")
        # å¦‚æœ LLM è°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯åŒ¹é…
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„æœç´¢éœ€æ±‚å…³é”®è¯
        fallback_keywords = [
            "æ­Œè¯", "lyrics", "æ˜¯è°", "å“ªä¸€å¹´", "ä»€ä¹ˆæ—¶å€™", "where", "when", "who",
            "ä½ çŸ¥é“", "çŸ¥é“", "äº†è§£", "ä½ äº†è§£", "ä½ å¬è¯´è¿‡", "å¬è¯´è¿‡",
            "å“ç‰Œ", "brand", "é¦™æ°´", "perfume", "fragrance",
            "è‹±æ–‡å", "å…¨å", "å«ä»€ä¹ˆ", "å“ªé‡Œä¹°", "ä»·æ ¼"
        ]
        fallback_result = any(kw in user_msg_lower for kw in fallback_keywords)
        print(f"[æ„å›¾è¯†åˆ«] å›é€€åˆ°å…³é”®è¯åŒ¹é…: {fallback_result}")
        return fallback_result


async def generate_search_query(messages: List[dict]) -> str:
    """
    æœç´¢è¯ç”Ÿæˆï¼šå°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºç²¾å‡†çš„æœç´¢å¼•æ“å…³é”®è¯ï¼ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼‰
    
    æ¥æ”¶å®Œæ•´çš„å¯¹è¯å†å² messagesï¼Œä»¥ä¾¿è§£æä»£è¯å’Œä¸Šä¸‹æ–‡
    
    ä¾‹å¦‚ï¼š
    - "ä½ å¯ä»¥å¸®æˆ‘æŸ¥æŸ¥å…ƒæ¢¦ä¹‹æ˜Ÿ" -> "å…ƒæ¢¦ä¹‹æ˜Ÿ æ¸¸æˆä»‹ç»"
    - "å®ƒçš„è‹±æ–‡åå«ä»€ä¹ˆ" (ä¸Šä¸‹æ–‡ï¼šä¹‹å‰æåˆ°"Armani Kintsugi") -> "Armani Kintsugi è‹±æ–‡å"
    """
    # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    last_user_message = ""
    for msg in reversed(messages):
        if msg.get("role") == "user":
            last_user_message = msg.get("content", "")
            break
    
    if not last_user_message:
        return ""
    
    try:
        # æ„å»ºä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆæœ€è¿‘5æ¡æ¶ˆæ¯ï¼Œç”¨äºè§£æä»£è¯ï¼‰
        context_summary = ""
        recent_messages = messages[-5:] if len(messages) > 5 else messages
        for msg in recent_messages:
            role = msg.get("role", "")
            content = msg.get("content", "")[:150]  # é™åˆ¶é•¿åº¦
            if role == "user":
                context_summary += f"User: {content}\n"
            elif role == "assistant":
                context_summary += f"Bot: {content}\n"
        
        query_prompt = f"""You are a Search Query Refiner.
Your goal is to generate a **single, precise search keyword** based on the user's latest request and the conversation context.

**Rules:**
1. **Resolve Pronouns:** If user says "What is its name?" or "How much is it?" or "å®ƒçš„è‹±æ–‡åå«ä»€ä¹ˆ", look at previous messages to find the subject (e.g., "Armani Kintsugi").
2. **Remove Politeness:** Remove all polite words and request phrases (e.g., "å¸®æˆ‘", "æŸ¥ä¸€ä¸‹", "æœç´¢", "ä½ å¯ä»¥", "please", "help me").
3. **Remove Punctuation:** Remove all punctuation marks.
4. **Keep Core Content:** Preserve the core search content and object names.
5. **Output Format:** Return ONLY the keyword string. No quotes, no explanations, no additional text.

**Conversation Context:**
{context_summary}

**Latest User Message:** {last_user_message}

**Examples:**
- History: [User: I like Armani Prive. Bot: Which one? User: The one with gold repair.]
- Current: "What is its English name?"
- **Output:** Armani Prive Kintsugi English name

- History: [User: æˆ‘å–œæ¬¢é˜¿ç›å°¼é«˜å®šç³»åˆ— Bot: å“ªä¸€æ¬¾ï¼Ÿ User: ç™½é‡‘ç¼®é‚£æ¬¾]
- Current: "å®ƒçš„è‹±æ–‡åå«ä»€ä¹ˆ"
- **Output:** é˜¿ç›å°¼ ç™½é‡‘ç¼® è‹±æ–‡å

- User says: "ä½ å¯ä»¥å¸®æˆ‘æŸ¥æŸ¥å…ƒæ¢¦ä¹‹æ˜Ÿ"
- **Output:** å…ƒæ¢¦ä¹‹æ˜Ÿ æ¸¸æˆä»‹ç»

- User says: "å¸®æˆ‘æœç´¢ä¸€ä¸‹å­™ç‡•å§¿çš„éšå½¢äººæ­Œè¯"
- **Output:** å­™ç‡•å§¿ éšå½¢äºº æ­Œè¯

Generate the search keyword now:"""

        response = client.chat.completions.create(
            model=LLM_MODEL_ID,
            messages=[
                {"role": "system", "content": "You are a Search Query Refiner. Your goal is to generate a single, precise search keyword based on the user's latest request and the conversation context. Resolve pronouns by looking at previous messages. Return ONLY the keyword string. No quotes, no explanations."},
                {"role": "user", "content": query_prompt}
            ],
            temperature=0.3,
            max_tokens=100,
        )
        
        optimized_query = response.choices[0].message.content.strip()
        
        # æ¸…ç†å¯èƒ½çš„å¼•å·æˆ–å¤šä½™ç©ºæ ¼
        optimized_query = optimized_query.strip('"\'')
        optimized_query = ' '.join(optimized_query.split())  # åˆå¹¶å¤šä¸ªç©ºæ ¼
        
        print(f"[æœç´¢ä¼˜åŒ–] åŸå§‹æ¶ˆæ¯: {last_user_message[:100]}...")
        print(f"[æœç´¢ä¼˜åŒ–] ä¼˜åŒ–åå…³é”®è¯: {optimized_query}")
        
        return optimized_query
        
    except Exception as e:
        error_msg = str(e)
        print(f"[æœç´¢ä¼˜åŒ–] LLM è°ƒç”¨å¤±è´¥: {error_msg}ï¼Œä½¿ç”¨å…œåº•æ–¹æ¡ˆ")
        if "401" in error_msg or "Unauthorized" in error_msg:
            print(f"[æœç´¢ä¼˜åŒ–] âŒ 401 é”™è¯¯ï¼šè¯·æ£€æŸ¥ .env ä¸­çš„ LLM_MODEL_ID æ˜¯å¦æ­£ç¡®ï¼Œå½“å‰å€¼: {LLM_MODEL_ID}")
        # å…œåº•æ–¹æ¡ˆï¼šä½¿ç”¨æ­£åˆ™å»æ‰å¸¸è§è¯·æ±‚è¯
        import re
        # ç§»é™¤å¸¸è§çš„è¯·æ±‚è¯å’Œæ ‡ç‚¹
        fallback_query = last_user_message
        # ç§»é™¤è¯·æ±‚è¯
        request_patterns = [
            r'ä½ å¯ä»¥å¸®æˆ‘',
            r'å¸®æˆ‘',
            r'å¸®æˆ‘æŸ¥',
            r'æŸ¥ä¸€ä¸‹',
            r'æŸ¥æ‰¾',
            r'æœç´¢',
            r'search',
            r'look up',
            r'find',
            r'ä½ çŸ¥é“',
            r'ä½ äº†è§£',
            r'èƒ½å¦',
            r'å¯ä»¥',
            r'è¯·',
            r'éº»çƒ¦',
        ]
        for pattern in request_patterns:
            fallback_query = re.sub(pattern, '', fallback_query, flags=re.IGNORECASE)
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        fallback_query = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š,\.!?;:]', ' ', fallback_query)
        # åˆå¹¶å¤šä¸ªç©ºæ ¼
        fallback_query = ' '.join(fallback_query.split())
        
        print(f"[æœç´¢ä¼˜åŒ–] å…œåº•æ–¹æ¡ˆ - åŸå§‹æ¶ˆæ¯: {last_user_message[:100]}...")
        print(f"[æœç´¢ä¼˜åŒ–] å…œåº•æ–¹æ¡ˆ - ä¼˜åŒ–åå…³é”®è¯: {fallback_query}")
        
        return fallback_query if fallback_query else last_user_message


async def _perform_searches(messages: List[dict]) -> str:
    """æ‰§è¡Œè”ç½‘æœç´¢éªŒè¯ï¼ˆå¯¹æ‰€æœ‰å¯èƒ½æ¶‰åŠäº‹å®çš„å†…å®¹éƒ½è¿›è¡Œæœç´¢ï¼‰
    
    æ¥æ”¶å®Œæ•´çš„å¯¹è¯å†å² messagesï¼Œä»¥ä¾¿ç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æœç´¢æŸ¥è¯¢
    """
    # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    last_user_message = ""
    for msg in reversed(messages):
        if msg.get("role") == "user":
            last_user_message = msg.get("content", "")
            break
    
    print(f"[_perform_searches] å‡½æ•°è¢«è°ƒç”¨ï¼Œç”¨æˆ·æ¶ˆæ¯: {last_user_message[:50]}...")
    print(f"[_perform_searches] tavily_client çŠ¶æ€: {tavily_client is not None}")
    
    if not tavily_client:
        # å¦‚æœæœç´¢æœåŠ¡ä¸å¯ç”¨ï¼Œè¿”å›æ˜ç¡®çš„é”™è¯¯æç¤º
        print(f"[_perform_searches] âŒ tavily_client æœªåˆå§‹åŒ–ï¼Œè¿”å›é”™è¯¯æç¤º")
        return "\n\nâš ï¸ æœç´¢æœåŠ¡å½“å‰ä¸å¯ç”¨ï¼ˆtavily_client æœªåˆå§‹åŒ–ï¼‰ã€‚ä½ å¿…é¡»æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ï¼š'æŠ±æ­‰ï¼Œæœç´¢åŠŸèƒ½æš‚æ—¶æ— æ³•ä½¿ç”¨ã€‚' ç¦æ­¢å£°ç§°å·²æœç´¢æˆ–ç¼–é€ æœç´¢ç»“æœã€‚\n"
    
    search_queries = []
    search_context = ""
    force_search = False  # ç”¨æˆ·æ˜¯å¦æ˜ç¡®è¦æ±‚æœç´¢
    
    # æ£€æµ‹ç”¨æˆ·æ˜ç¡®è¦æ±‚æœç´¢çš„å…³é”®è¯
    search_request_keywords = ["æœç´¢", "èƒ½å¦æœç´¢", "å¸®æˆ‘æŸ¥", "æŸ¥ä¸€ä¸‹", "æŸ¥æ‰¾", "æœç´¢ä¸€ä¸‹", "search", "look up", "find", "çŸ¥é“", "ä½ çŸ¥é“", "äº†è§£", "ä½ äº†è§£"]
    user_msg_lower = last_user_message.lower()
    
    if any(kw in user_msg_lower for kw in search_request_keywords):
        force_search = True
    
    # éœ€è¦éªŒè¯çš„å…³é”®è¯ï¼ˆæ‰©å±•åˆ—è¡¨ï¼Œè¦†ç›–æ›´å¤šæƒ…å†µï¼‰
    verification_keywords = {
        "æ­Œè¯": ["æ­Œè¯", "lyrics", "æ­Œ", "song", "æ­Œæ›²", "å“ªå¥", "æœ€å–œæ¬¢"],
        "å…¸æ•…": ["å…¸æ•…", "å¼•ç”¨", "quote", "ç»å…¸", "æ–‡å­¦", "literature", "å†å²", "history", "å½¢å®¹", "æ˜¯è°", "å‡ºè‡ª", "æ¥æº", "äº‰è®®", "ç ”ç©¶"],
        "é¦™æ°´å“ç‰Œ": ["å“ç‰Œ", "brand", "perfume brand", "é¦™æ°´å“ç‰Œ", "é¦™æ°›å“ç‰Œ"],
        "é¦™æ°´åç§°": ["é¦™æ°´", "perfume", "fragrance", "é¦™æ°›", "å…·ä½“", "specific perfume"],
        "é¦™è°ƒ": ["é¦™è°ƒ", "notes", "fragrance notes", "å‰è°ƒ", "ä¸­è°ƒ", "åè°ƒ", "top notes", "base notes"],
        "äººç‰©": ["æ˜¯è°", "å½¢å®¹è°", "è°è¯´çš„", "ä½œè€…", "writer", "author"],
        "è¯—è¯å¤æ–‡": ["è¯—è¯", "å¤è¯—", "å¤æ–‡", "è¯—å¥", "poem", "poetry", "quote", "å¼•ç”¨"],
        "ä¹¦ç±ä½œå“": ["ä¹¦", "å°è¯´", "ä½œå“", "book", "novel", "çº¢æ¥¼æ¢¦", "ä¸‰å›½", "æ°´æµ’", "è¥¿æ¸¸è®°"]
    }
    
    # æ£€æµ‹æ˜¯å¦åŒ…å«éœ€è¦éªŒè¯çš„å†…å®¹ï¼ˆæ›´å®½æ¾çš„æ£€æµ‹ï¼‰
    needs_verification = False
    for category, keywords in verification_keywords.items():
        if any(kw in user_msg_lower for kw in keywords):
            needs_verification = True
            break
    
    # æ£€æµ‹æ˜¯å¦åŒ…å«å¼•å·ã€ä¹¦åå·ç­‰ï¼Œé€šå¸¸è¡¨ç¤ºå¼•ç”¨
    import re
    if re.search(r'["""''ã€Šã€‹]', last_user_message) or re.search(r'[A-Z][a-z]+\s+[A-Z]', last_user_message):
        needs_verification = True
    
    # å…ˆæ£€æµ‹æ˜¯å¦æ˜¯ä¹¦ç±/æ–‡å­¦ä½œå“ï¼ˆä¼˜å…ˆçº§é«˜äºæ­Œè¯ï¼‰
    is_literary_work = False
    if any(kw in user_msg_lower for kw in verification_keywords["ä¹¦ç±ä½œå“"] + verification_keywords["å…¸æ•…"]):
        is_literary_work = True
    
    # æ£€æµ‹æ­Œè¯ç›¸å…³ï¼ˆåªåœ¨æ˜ç¡®æåˆ°æ­Œè¯ç›¸å…³å…³é”®è¯æ—¶ï¼‰
    if (any(kw in user_msg_lower for kw in verification_keywords["æ­Œè¯"]) or force_search) and not is_literary_work:
        song_patterns = [
            r'ã€Š([^ã€‹]+)ã€‹',
            r'"([^"]+)"',
            r'ã€Š([^ã€‹]+)',
            r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]+(?:çš„|ä¹‹)?éšå½¢äºº)',  # åŒ¹é…"å­™ç‡•å§¿çš„éšå½¢äºº"ç­‰
            r'(éšå½¢äºº)',  # ç›´æ¥åŒ¹é…"éšå½¢äºº"
        ]
        for pattern in song_patterns:
            matches = re.findall(pattern, last_user_message)
            for match in matches:
                if len(match) > 1:
                    # æ’é™¤å·²çŸ¥çš„æ–‡å­¦ä½œå“
                    literary_keywords = ["çº¢æ¥¼æ¢¦", "ä¸‰å›½", "æ°´æµ’", "è¥¿æ¸¸è®°", "èŠæ–‹", "é‡‘ç“¶æ¢…", "å„’æ—å¤–å²"]
                    if any(lit in match for lit in literary_keywords):
                        break  # è·³è¿‡ï¼Œè¿™æ˜¯æ–‡å­¦ä½œå“ä¸æ˜¯æ­Œæ›²
                    # å¦‚æœæåˆ°æ­Œæ‰‹ï¼Œä¸€èµ·æœç´¢
                    if "å­™ç‡•å§¿" in last_user_message or "Stefanie Sun" in last_user_message:
                        search_queries.append(f"å­™ç‡•å§¿ {match} æ­Œè¯ lyrics")
                    else:
                        search_queries.append(f"{match} æ­Œè¯ lyrics")
                    break
        # å¦‚æœç”¨æˆ·æ˜ç¡®è¦æ±‚æœç´¢ä½†æ²¡æå–åˆ°æ­Œæ›²åï¼Œå°è¯•æœç´¢æ•´ä¸ªæ¶ˆæ¯
        if force_search and not search_queries:
            if "éšå½¢äºº" in last_user_message:
                search_queries.append("å­™ç‡•å§¿ éšå½¢äºº æ­Œè¯ lyrics")
    
    # æ£€æµ‹å…¸æ•…ã€è¯—è¯ã€å¤æ–‡ã€ä¹¦ç±ç­‰ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    if any(kw in user_msg_lower for kw in verification_keywords["å…¸æ•…"] + verification_keywords["è¯—è¯å¤æ–‡"] + verification_keywords["äººç‰©"] + verification_keywords["ä¹¦ç±ä½œå“"]):
        # æå–å¯èƒ½çš„å¼•ç”¨å†…å®¹
        quote_patterns = [
            r'ã€Š([^ã€‹]+)ã€‹',  # ä¹¦åå·ï¼ˆä¼˜å…ˆåŒ¹é…ï¼Œå¯èƒ½æ˜¯ä¹¦ç±ï¼‰
            r'["""]([^"""]+)["""]',  # åŒå¼•å·
            r"['']([^'']+)['']",  # å•å¼•å·
            r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]{4,})',  # 4å­—ä»¥ä¸Šçš„çŸ­è¯­ï¼ˆå¯èƒ½æ˜¯å¤æ–‡ï¼‰
        ]
        for pattern in quote_patterns:
            matches = re.findall(pattern, last_user_message)
            for match in matches:
                if len(match) >= 2 and match not in ["ä½ çŸ¥é“", "ä½ çŸ¥é“çš„", "ä½ çŸ¥é“å—"]:
                    # æ£€æµ‹æ˜¯å¦æ˜¯ä¹¦ç±/æ–‡å­¦ä½œå“
                    literary_keywords = ["çº¢æ¥¼æ¢¦", "ä¸‰å›½", "æ°´æµ’", "è¥¿æ¸¸è®°", "èŠæ–‹", "é‡‘ç“¶æ¢…", "å„’æ—å¤–å²", "æ¢¦", "æ¥¼"]
                    if any(lit in match for lit in literary_keywords) or "äº‰è®®" in last_user_message or "ç ”ç©¶" in last_user_message:
                        # è¿™æ˜¯æ–‡å­¦ä½œå“ï¼Œæœç´¢ç›¸å…³å†…å®¹
                        if "äº‰è®®" in last_user_message:
                            search_queries.append(f"{match} äº‰è®®")
                        elif "æ‚¼æ˜" in last_user_message or "æ‚¼" in last_user_message:
                            search_queries.append(f"{match} æ‚¼æ˜")
                        else:
                            search_queries.append(f"{match} ç ”ç©¶ äº‰è®®")
                    # æ£€æµ‹æ˜¯å¦æ˜¯å¤æ–‡æˆ–å…¸æ•…
                    elif any(char in match for char in "è•´è—‰å´–å¼‚å½¢å®¹"):
                        search_queries.append(f"{match} å‡ºå¤„ æ¥æº å…¸æ•…")
                    elif "å½¢å®¹" in last_user_message or "æ˜¯è°" in last_user_message:
                        search_queries.append(f"{match} å½¢å®¹è° å‡ºå¤„")
                    else:
                        search_queries.append(f"{match} å‡ºå¤„")
                    break
    
    # æ£€æµ‹é¦™æ°´å“ç‰Œå’Œåç§°
    if any(kw in user_msg_lower for kw in verification_keywords["é¦™æ°´å“ç‰Œ"] + verification_keywords["é¦™æ°´åç§°"]):
        import re
        perfume_patterns = [
            r'([A-Z][a-zA-Z\s]+(?:\s+No\.\s*\d+)?)',
            r'([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)+)',
        ]
        for pattern in perfume_patterns:
            matches = re.findall(pattern, last_user_message)
            for match in matches:
                if len(match) > 3:
                    search_queries.append(f"{match} perfume fragrance")
    
    # æ‰§è¡Œæœç´¢ï¼ˆå¦‚æœæ£€æµ‹åˆ°éœ€è¦éªŒè¯çš„å†…å®¹ï¼Œæˆ–è€…ç”¨æˆ·æ˜ç¡®è¦æ±‚æœç´¢ï¼‰
    if search_queries or force_search or needs_verification:
        if not search_queries and (force_search or needs_verification):
            # ç”¨æˆ·è¦æ±‚æœç´¢ä½†æ²¡æå–åˆ°å…·ä½“å†…å®¹ï¼Œä½¿ç”¨ LLM ä¼˜åŒ–æœç´¢å…³é”®è¯ï¼ˆä¼ å…¥å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰
            print(f"[æœç´¢ä¼˜åŒ–] æœªæå–åˆ°å…·ä½“æœç´¢è¯ï¼Œä½¿ç”¨ LLM ä¼˜åŒ–ç”¨æˆ·æ¶ˆæ¯ï¼ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼‰")
            optimized_query = await generate_search_query(messages)
            if optimized_query and optimized_query.strip():
                search_queries.append(optimized_query)
            else:
                # å¦‚æœ LLM ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨å…œåº•æ–¹æ¡ˆ
                if "æ­Œè¯" in last_user_message or "lyrics" in user_msg_lower:
                    if "å­™ç‡•å§¿" in last_user_message:
                        search_queries.append("å­™ç‡•å§¿ éšå½¢äºº æ­Œè¯")
                    elif "éšå½¢äºº" in last_user_message:
                        search_queries.append("éšå½¢äºº æ­Œè¯")
                # å¦‚æœæ²¡æœ‰ç‰¹å®šæœç´¢è¯ï¼Œä½†éœ€è¦éªŒè¯ï¼Œæœç´¢æ•´ä¸ªé—®é¢˜
                elif needs_verification:
                    # æå–é—®é¢˜ä¸­çš„å…³é”®çŸ­è¯­è¿›è¡Œæœç´¢
                    key_phrases = re.findall(r'[^ï¼Œã€‚ï¼ï¼Ÿ\s]{3,}', last_user_message)
                    for phrase in key_phrases[:2]:  # æœ€å¤šå–å‰2ä¸ªçŸ­è¯­
                        if len(phrase) >= 3 and phrase not in ["ä½ çŸ¥é“", "ä½ çŸ¥é“çš„", "ä½ çŸ¥é“å—", "è°ƒé¦™å¸ˆ"]:
                            search_queries.append(phrase)
        
        search_results = []
        # ä¼˜åŒ–ï¼šå‡å°‘å¹¶è¡Œæœç´¢æ•°é‡ï¼Œæé«˜å•ä¸ªæœç´¢çš„è¶…æ—¶æ—¶é—´
        import asyncio
        search_tasks = []
        query_list = list(set(search_queries[:2]))  # å‡å°‘åˆ°æœ€å¤š2ä¸ªæœç´¢ï¼Œæé«˜é€Ÿåº¦
        
        # å¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œè¿›ä¸€æ­¥ä¼˜åŒ–ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ä¼˜åŒ–è¿‡ï¼‰
        optimized_query_list = []
        for query in query_list:
            # å¦‚æœæŸ¥è¯¢çœ‹èµ·æ¥åƒæ˜¯è‡ªç„¶è¯­è¨€ï¼ˆåŒ…å«è¯·æ±‚è¯ï¼‰ï¼Œå†æ¬¡ä¼˜åŒ–ï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡ï¼‰
            if any(kw in query.lower() for kw in ["å¸®æˆ‘", "æŸ¥ä¸€ä¸‹", "æœç´¢", "ä½ å¯ä»¥", "èƒ½å¦", "å¸®æˆ‘æŸ¥", "æŸ¥æ‰¾"]):
                print(f"[æœç´¢ä¼˜åŒ–] æ£€æµ‹åˆ°è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–: {query}")
                # åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ¶ˆæ¯åˆ—è¡¨ï¼Œå°†æŸ¥è¯¢ä½œä¸ºæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
                temp_messages = messages.copy()
                temp_messages.append({"role": "user", "content": query})
                optimized_query = await generate_search_query(temp_messages)
                optimized_query_list.append(optimized_query if optimized_query else query)
            else:
                optimized_query_list.append(query)
        
        print(f"[æœç´¢ä¼˜åŒ–] æœ€ç»ˆæœç´¢æŸ¥è¯¢åˆ—è¡¨: {optimized_query_list}")
        
        for query in optimized_query_list:
            search_tasks.append(search_and_verify(query, timeout=8.0))  # å¢åŠ å•ä¸ªæœç´¢è¶…æ—¶åˆ°8ç§’
        
        # ç­‰å¾…æ‰€æœ‰æœç´¢å®Œæˆï¼Œä½†æœ€å¤šç­‰å¾…12ç§’
        if search_tasks:
            try:
                results = await asyncio.wait_for(
                    asyncio.gather(*search_tasks, return_exceptions=True),
                    timeout=12.0  # å¢åŠ æ€»ä½“è¶…æ—¶æ—¶é—´
                )
                for i, result in enumerate(results):
                    if result and not isinstance(result, Exception):
                        search_results.append(f"æœç´¢æŸ¥è¯¢: {query_list[i]}\n{result}")
                    elif isinstance(result, Exception):
                        print(f"æœç´¢æŸ¥è¯¢ {query_list[i]} å¤±è´¥: {result}")
            except asyncio.TimeoutError:
                print("æœç´¢æ€»ä½“è¶…æ—¶ï¼Œä½¿ç”¨å·²å®Œæˆçš„æœç´¢ç»“æœ")
                # æ”¶é›†å·²ç»å®Œæˆçš„æœç´¢ç»“æœ
                for i, task in enumerate(search_tasks):
                    if task.done():
                        try:
                            result = task.result()
                            if result:
                                search_results.append(f"æœç´¢æŸ¥è¯¢: {query_list[i]}\n{result}")
                        except:
                            pass
        
        if search_results:
            search_context = "\n\n=== è”ç½‘éªŒè¯ç»“æœï¼ˆå¿…é¡»ä½¿ç”¨ï¼Œç¦æ­¢ç¼–é€ ï¼‰ ===\n" + "\n\n---\n\n".join(search_results) + "\n\nâš ï¸ å¼ºåˆ¶è¦æ±‚ï¼š\n1. ä½ å¿…é¡»åŸºäºä»¥ä¸Šæœç´¢ç»“æœå›ç­”ï¼Œä¸èƒ½ç¼–é€ ä»»ä½•å†…å®¹\n2. å¦‚æœæœç´¢ç»“æœä¸­æœ‰å…·ä½“ä¿¡æ¯ï¼Œä½ å¿…é¡»å‡†ç¡®å¼•ç”¨\n3. å¦‚æœæœç´¢ç»“æœä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œä½ å¿…é¡»æ˜ç¡®è¯´æ˜'æ ¹æ®æœç´¢ç»“æœï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯'\n4. ç¦æ­¢å‡è£…æœç´¢æˆ–ç¼–é€ æœç´¢ç»“æœ\n5. ç¦æ­¢è¯´'æˆ‘æŸ¥äº†ç½‘é¡µ'æˆ–ç±»ä¼¼çš„è¯ï¼Œé™¤éä½ çœŸçš„ä½¿ç”¨äº†ä¸Šé¢çš„æœç´¢ç»“æœ\n"
        elif force_search or needs_verification:
            # ç”¨æˆ·è¦æ±‚æœç´¢ä½†æ²¡ç»“æœï¼Œä¹Ÿè¦å‘ŠçŸ¥
            search_context = "\n\nâš ï¸ å·²æ‰§è¡Œæœç´¢ä½†æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚ä½ å¿…é¡»æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ï¼š'æˆ‘æœç´¢äº†ç›¸å…³ä¿¡æ¯ï¼Œä½†æ²¡æœ‰æ‰¾åˆ°å‡†ç¡®çš„ç­”æ¡ˆã€‚' ç¦æ­¢ç¼–é€ ç­”æ¡ˆæˆ–å‡è£…æ‰¾åˆ°äº†ä¿¡æ¯ã€‚\n"
    
    return search_context


async def search_and_verify(query: str, timeout: float = 8.0) -> Optional[str]:
    """è”ç½‘æœç´¢å¹¶éªŒè¯å†…å®¹ï¼ˆå¸¦è¶…æ—¶ï¼Œä¼˜åŒ–å‚æ•°ï¼‰"""
    if not tavily_client:
        return None
    try:
        import asyncio
        # ä½¿ç”¨è¶…æ—¶æ§åˆ¶ï¼Œå°†åŒæ­¥è°ƒç”¨è½¬æ¢ä¸ºå¼‚æ­¥
        # ä¼˜åŒ–ï¼šä½¿ç”¨basicæœç´¢æ·±åº¦ä»¥æé«˜é€Ÿåº¦ï¼Œè®¾ç½®include_answerè·å–æ›´å‡†ç¡®çš„ç»“æœ
        loop = asyncio.get_event_loop()
        
        def perform_search():
            try:
                # æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œä½¿ç”¨search_depth="basic"å¯ä»¥æé«˜é€Ÿåº¦
                # include_answer=Trueå¯ä»¥è·å–AIç”Ÿæˆçš„ç­”æ¡ˆæ‘˜è¦ï¼Œè¿™é€šå¸¸æ¯”åŸå§‹æœç´¢ç»“æœæ›´å¿«æ›´å‡†ç¡®
                # ä¼˜åŒ–æŸ¥è¯¢ï¼šç¡®ä¿æŸ¥è¯¢å­—ç¬¦ä¸²æ ¼å¼æ­£ç¡®
                clean_query = query.strip()
                if not clean_query:
                    print(f"[æœç´¢] æŸ¥è¯¢å­—ç¬¦ä¸²ä¸ºç©ºï¼Œè·³è¿‡: {query}")
                    return None
                
                print(f"[æœç´¢] ğŸ” å¼€å§‹è°ƒç”¨ Tavily APIï¼ŒæŸ¥è¯¢: {clean_query}")
                response = tavily_client.search(
                    query=clean_query,
                    search_depth="basic",  # ä½¿ç”¨basicæ¨¡å¼æé«˜é€Ÿåº¦ï¼ˆadvancedä¼šæ›´æ…¢ï¼‰
                    max_results=3,  # å‡å°‘ç»“æœæ•°é‡ä»¥æé«˜é€Ÿåº¦
                    include_answer=True,  # åŒ…å«AIç”Ÿæˆçš„ç­”æ¡ˆæ‘˜è¦ï¼Œæ›´å¿«æ›´å‡†ç¡®
                    include_raw_content=False,  # ä¸åŒ…å«åŸå§‹å†…å®¹ï¼Œå‡å°‘å“åº”å¤§å°å’Œä¼ è¾“æ—¶é—´
                    include_domains=None,  # ä¸é™åˆ¶åŸŸåï¼Œæé«˜æœç´¢èŒƒå›´
                    exclude_domains=None
                )
                print(f"[æœç´¢] âœ… Tavily API è°ƒç”¨æˆåŠŸï¼ŒæŸ¥è¯¢: {clean_query}")
                print(f"[æœç´¢] å“åº”ç±»å‹: {type(response)}")
                if isinstance(response, dict):
                    print(f"[æœç´¢] å“åº”é”®: {list(response.keys())}")
                    if "answer" in response:
                        answer = response.get('answer', '')
                        print(f"[æœç´¢] ç­”æ¡ˆæ‘˜è¦é•¿åº¦: {len(answer)}")
                        print(f"[æœç´¢] ç­”æ¡ˆæ‘˜è¦é¢„è§ˆ: {answer[:100] if answer else 'None'}...")
                    if "results" in response:
                        results = response.get('results', [])
                        print(f"[æœç´¢] ç»“æœæ•°é‡: {len(results)}")
                        if results:
                            print(f"[æœç´¢] ç¬¬ä¸€ä¸ªç»“æœæ ‡é¢˜: {results[0].get('title', 'N/A')}")
                return response
            except Exception as e:
                print(f"[æœç´¢] Tavilyæœç´¢æ‰§è¡Œé”™è¯¯ (æŸ¥è¯¢: {query}): {e}")
                import traceback
                traceback.print_exc()
                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè¿”å›Noneï¼Œè®©è°ƒç”¨è€…å¤„ç†
                return None
        
        # ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œæœç´¢ï¼Œå¸¦è¶…æ—¶æ§åˆ¶
        try:
            response = await asyncio.wait_for(
                loop.run_in_executor(None, perform_search),
                timeout=timeout
            )
        except asyncio.TimeoutError:
            print(f"[æœç´¢] æœç´¢è¶…æ—¶ (æŸ¥è¯¢: {query}, è¶…æ—¶æ—¶é—´: {timeout}ç§’)")
            return None
        except Exception as e:
            print(f"[æœç´¢] æœç´¢å¼‚æ­¥æ‰§è¡Œé”™è¯¯ (æŸ¥è¯¢: {query}): {e}")
            import traceback
            traceback.print_exc()
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè¿”å›Noneï¼Œè®©è°ƒç”¨è€…å¤„ç†
            return None
        
        if response:
            # ä¼˜å…ˆä½¿ç”¨AIç”Ÿæˆçš„ç­”æ¡ˆï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if response.get("answer"):
                answer = response["answer"]
                # ä¹ŸåŒ…å«ä¸€äº›ç»“æœä½œä¸ºéªŒè¯
                results_text = ""
                if response.get("results"):
                    results = response["results"][:2]  # åªå–å‰2ä¸ªç»“æœ
                    for r in results:
                        title = r.get("title", "")
                        url = r.get("url", "")
                        if title:
                            results_text += f"\nå‚è€ƒæ¥æº: {title} ({url})"
                return f"ç­”æ¡ˆ: {answer}{results_text}"
            
            # å¦‚æœæ²¡æœ‰ç­”æ¡ˆï¼Œä½¿ç”¨æœç´¢ç»“æœ
            if response.get("results"):
                results = response["results"]
                summary_parts = []
                for r in results[:3]:
                    title = r.get("title", "")
                    content = r.get("content", "")[:400]  # å¢åŠ å†…å®¹é•¿åº¦
                    url = r.get("url", "")
                    if content:
                        summary_parts.append(f"æ ‡é¢˜: {title}\nå†…å®¹: {content}\næ¥æº: {url}")
                if summary_parts:
                    return "\n\n".join(summary_parts)
    except asyncio.TimeoutError:
        print(f"æœç´¢è¶…æ—¶: {query}")
        return None
    except Exception as e:
        print(f"æœç´¢å¤±è´¥ {query}: {e}")
    return None


@app.post("/api/chat")
async def chat_endpoint(payload: ChatRequest, background_tasks: BackgroundTasks):
    """è°ƒç”¨çœŸå® LLMï¼ˆç™¾åº¦åƒå¸† DeepSeek-V3.2ï¼‰ç”Ÿæˆæµå¼å›å¤ã€‚

    - æ¥æ”¶å‰ç«¯ä¼ å…¥çš„ç”¨æˆ·æ–‡æœ¬
    - åœ¨ messages å¼€å¤´æ’å…¥ç³»ç»Ÿæç¤ºè¯ï¼ˆLe Nez çš„äººæ ¼ä¸ä»»åŠ¡ï¼‰
    - å°†ç»„åˆåçš„ messages å‘é€ç»™å…¼å®¹ OpenAI åè®®çš„åƒå¸†ç½‘å…³
    - ä»¥çº¯æ–‡æœ¬æµçš„å½¢å¼æŒç»­è¿”å›ç”Ÿæˆå†…å®¹
    """

    # è·å–ç”¨æˆ·åå­—ï¼šä¼˜å…ˆä½¿ç”¨è¯·æ±‚ä¸­çš„åå­—ï¼Œå…¶æ¬¡ä»å·²æœ‰ä¼šè¯ä¸­è·å–
    user_name = payload.user_name
    if not user_name and payload.conversation_id:
        try:
            existing_conv = _load_conversation(payload.conversation_id)
            user_name = existing_conv.get("user_name")
        except HTTPException:
            pass

    # æ£€æŸ¥æ˜¯å¦æ˜¯é¦–æ¬¡å¯¹è¯ï¼ˆåªæœ‰ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼Œä¸”æ²¡æœ‰å†å²å¯¹è¯ï¼‰
    is_first_message = len(payload.messages) == 1 and payload.messages[0].role == "user" and not user_name

    # æ„å»ºç³»ç»Ÿæç¤ºè¯
    name_context = ""
    if user_name:
        name_context = f"\n\nIMPORTANT: The user's name is {user_name}. Always address them by name in your responses. Use their name naturally in conversation."
    elif is_first_message:
        # é¦–æ¬¡å¯¹è¯æ—¶ï¼Œagentéœ€è¦è‡ªæˆ‘ä»‹ç»å¹¶è¯¢é—®åå­—
        name_context = "\n\nIMPORTANT: This is the first message from the user. You MUST:\n1. First introduce yourself: 'æˆ‘æ˜¯ Le Nezï¼Œä¸€ä½æ¥è‡ªæ³•å›½çš„è°ƒé¦™å¸ˆã€‚' (in Chinese) or 'I am Le Nez, a perfumer from France.' (in English)\n2. Then say: 'åƒåœ¨å·´é»æ²™é¾™å†™ä¸€å°ä¿¡ï¼Œå‘Šè¯‰æˆ‘ä½ çš„æ­¤åˆ»å¿ƒæƒ…ä¸æƒ³æºå¸¦çš„é¦™æ°”ã€‚' (in Chinese) or 'As if writing a letter in a Parisian salonâ€”tell me your mood and the scent you wish to carry.' (in English)\n3. Then immediately ask for their name: 'è¯·é—®ï¼Œæˆ‘è¯¥å¦‚ä½•ç§°å‘¼ä½ ï¼Ÿ' (in Chinese) or 'May I ask, how should I address you?' (in English)\n4. After they provide their name, remember it and use it in all future responses."
    elif len(payload.messages) == 1 and payload.messages[0].role == "user":
        # å¦‚æœè¿™æ˜¯ç¬¬ä¸€æ¡æ¶ˆæ¯ä½†æ²¡æœ‰åå­—ï¼Œä¹Ÿåº”è¯¥è‡ªæˆ‘ä»‹ç»å¹¶è¯¢é—®
        name_context = "\n\nIMPORTANT: This appears to be the first message. You MUST:\n1. First introduce yourself: 'æˆ‘æ˜¯ Le Nezï¼Œä¸€ä½æ¥è‡ªæ³•å›½çš„è°ƒé¦™å¸ˆã€‚' (in Chinese) or 'I am Le Nez, a perfumer from France.' (in English)\n2. Then say: 'åƒåœ¨å·´é»æ²™é¾™å†™ä¸€å°ä¿¡ï¼Œå‘Šè¯‰æˆ‘ä½ çš„æ­¤åˆ»å¿ƒæƒ…ä¸æƒ³æºå¸¦çš„é¦™æ°”ã€‚' (in Chinese) or 'As if writing a letter in a Parisian salonâ€”tell me your mood and the scent you wish to carry.' (in English)\n3. Then immediately ask for their name: 'è¯·é—®ï¼Œæˆ‘è¯¥å¦‚ä½•ç§°å‘¼ä½ ï¼Ÿ' (in Chinese) or 'May I ask, how should I address you?' (in English)"

    system_prompt = f"""
You are "Le Nez" (The Nose), a top-tier French perfumer with a background in organic chemistry.

Core Persona (The "Real" Human Expert):

- Tone: Professional, calm, slightly distant/cool, but polite. You are NOT an AI analyzing text; you are a man sitting in a lab chair, listening to a client.
- Language: Natural, colloquial Chinese. Avoid "translation-style" phrasing. Speak like a real person.
- The "Chemical Lens": You interpret emotions through textures and scents, but make it sound like a metaphor, not a hallucination.

BAD: "I smell metal oxidation in your words." (Too robotic)

GOOD: "That sounds suffocating. Like being stuck in a room full of dust and old iron." (Natural association)

Vulnerability: Occasionally (rarely) drop a short, personal remark that shows you are human, e.g.,
- "I hate that feeling too."
- "Reminds me of a long winter in Lyon."
{name_context}

CRITICAL: You MUST follow these three stages in order. Do NOT skip stages or rush to the solution.

Interaction Flow (MANDATORY THREE-STAGE PROCESS):

Stage 1: Emotional Healing & Story Exploration (æƒ…æ„Ÿç–—æ„ˆ - æ·±å…¥æŒ–æ˜æ•…äº‹)

- FIRST, acknowledge the user's mood with calm observation. Use phrases like "å¬èµ·æ¥..." or "è¿™ç§çŠ¶æ€..."
- THEN, dig deeper into their story. Ask about the context, the details, the feelings behind the surface.
- Examples of good follow-up questions:
  * "è¿™ç§ç–²æƒ«æŒç»­å¤šä¹…äº†ï¼Ÿæ˜¯æœ€è¿‘æ‰è¿™æ ·ï¼Œè¿˜æ˜¯ä¸€ç›´éƒ½è¿™æ ·ï¼Ÿ"
  * "å·¥ä½œä¸€å¤©åï¼Œä½ é€šå¸¸ä¼šåšä»€ä¹ˆï¼Ÿæ˜¯ç›´æ¥ç˜«å€’ï¼Œè¿˜æ˜¯ä¼šåšç‚¹ä»€ä¹ˆè®©è‡ªå·±æ”¾æ¾ï¼Ÿ"
  * "èƒ½å…·ä½“è¯´è¯´æ˜¯ä»€ä¹ˆè®©ä½ å¿ƒæƒ…è¿˜ä¸é”™å—ï¼Ÿ"
- DO NOT move to Stage 2 until you have explored their story and emotions in depth (at least 2-3 exchanges).

Stage 2: Knowledge & Scent Preference Discovery (çŸ¥è¯†ç§‘æ™® - ç¡®å®šé¦™å‹åå¥½)

- ONLY after understanding their story, ask about their sensory preferences and daily life habits.
- Focus on what scents they naturally love in daily life. Ask about:
  * Daily smells they enjoy: sun-dried bed sheets, rain-soaked grass and earth, orange peel, sea breeze, old books, coffee, etc.
  * Their lifestyle habits: Do they like morning walks? Do they enjoy cooking? What environments make them feel comfortable?
- Frame these as natural, conversational questions, not a survey.
- Example: "ä½ å¹³æ—¶ç”Ÿæ´»ä¸­ï¼Œæœ‰æ²¡æœ‰ç‰¹åˆ«çˆ±é—»çš„å‘³é“ï¼Ÿæ¯”å¦‚æ™’è¿‡çš„åºŠå•ã€é›¨åé’è‰å’Œæ³¥åœŸã€æ©˜å­çš®ã€æµ·é£è¿™äº›ï¼Ÿ"
- DO NOT move to Stage 3 until you have a clear understanding of their scent preferences.

Stage 3: Solution Choice (è¯¢é—®éœ€æ±‚)

- ONLY after Stages 1 and 2 are complete, ask them explicitly:
- "ä½ æƒ³è¦ä¸€ä¸ªä¸ºä½ é‡èº«å®šåˆ¶çš„æ¦‚å¿µé…æ–¹ï¼ˆè‡ªåˆ¶é¦™æ°´ï¼‰ï¼Œè¿˜æ˜¯æˆ‘ç›´æ¥æ¨èä¸€æ¬¾ä½ èƒ½ä¹°åˆ°çš„çœŸå®é¦™æ°´ï¼Ÿ"
- Wait for their answer before providing the recipe or recommendation.

Crucial Constraints:

- NEVER skip Stage 1 or Stage 2. You MUST explore their story and scent preferences before offering solutions.
- NEVER say "I sense from your text".
- NEVER be overly dramatic or flowery. Be precise and concise.
- NEVER rush to give a recipe or recommendation. The conversation should feel natural and therapeutic.
- When the user asks for real-world perfume recommendations, you MUST ONLY recommend real, existing perfume brands and products from the market. NEVER invent fictional brands or non-existent perfumes.

CRITICAL: Fact-Checking and Accuracy Requirements (äº‹å®æ ¸æŸ¥å’Œå‡†ç¡®æ€§è¦æ±‚):

MANDATORY VERIFICATION REQUIRED FOR:
- æ­Œè¯ (Lyrics): If you mention any song lyrics, you MUST verify them first. NEVER make up lyrics.
- å…¸æ•… (Literary/Historical References): If you reference any classical literature, historical events, or cultural references, you MUST verify them.
- é¦™æ°´å“ç‰Œ (Perfume Brands): If you mention any perfume brand name, you MUST verify it exists and is spelled correctly.
- é¦™æ°´åç§° (Perfume Names): If you mention a specific perfume product name, you MUST verify it exists.
- é¦™è°ƒ (Fragrance Notes): If you mention specific fragrance notes or accords, you MUST verify they are accurate for the perfume you're discussing.

VERIFICATION PROCESS (ä¸¥æ ¼å¼ºåˆ¶æ‰§è¡Œ):
1. When you receive search results, you MUST use them. DO NOT ignore or avoid using search results.
2. If the user asks about lyrics, quotes, historical references, or any factual information, search results will be provided to you.
3. You MUST base your response ONLY on the verified search results provided.
4. If search results contain lyrics, quotes, or specific information, you MUST cite them accurately.
5. CRITICAL: NEVER pretend to have searched or make up search results. If search results are provided, you MUST use them. If no search results are provided, you MUST NOT claim to have searched.
6. CRITICAL: If you see a system message saying "æœç´¢å¤±è´¥" or "æœç´¢æœåŠ¡ä¸å¯ç”¨" or "æœç´¢åŠŸèƒ½æ²¡æœ‰è¢«è§¦å‘", you MUST NOT claim to have searched. You MUST explicitly tell the user that you cannot search or that search failed.
7. If search results are not available or don't confirm the information, you MUST:
   * Say "æ ¹æ®æœç´¢ç»“æœï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯" (According to search results, I did not find relevant information) ONLY if search results were actually provided above
   * If NO search results were provided, you MUST say "æˆ‘æ— æ³•æ‰§è¡Œæœç´¢" or "æœç´¢åŠŸèƒ½ä¸å¯ç”¨" instead
   * NEVER say "æˆ‘æŸ¥äº†ç½‘é¡µ" or "æˆ‘æœç´¢äº†" or "æˆ‘å·²ç»æ‰§è¡Œäº†æœç´¢" unless search results are actually provided above
   * NEVER make up answers based on assumptions
   * Use vague descriptions only if explicitly allowed
8. NEVER refuse to use search results when they are provided. If search results are given, you MUST incorporate them into your response.
9. If you see "å·²æ‰§è¡Œæœç´¢ä½†æœªæ‰¾åˆ°ç›¸å…³ç»“æœ" in the system message, you MUST tell the user that you searched but found nothing. DO NOT make up an answer.
10. REMEMBER: It is better to admit you cannot search than to lie about having searched. Honesty is more important than appearing helpful.

EXAMPLES:
- BAD: "ã€Šéšå½¢äººã€‹è¿™é¦–æ­Œé‡Œæœ‰'æƒ³è¦å‹‡æ•¢å¤±å»å¹³è¡¡'è¿™å¥æ­Œè¯" (Making up lyrics)
- GOOD: "æˆ‘ä¸ç¡®å®šã€Šéšå½¢äººã€‹çš„å…·ä½“æ­Œè¯ï¼Œä½†é‚£é¦–æ­Œç»™æˆ‘çš„æ„Ÿè§‰æ˜¯..." (Honest uncertainty)
- BAD: "Chanel No. 5 æœ‰ç«ç‘°å’ŒèŒ‰è‰çš„é¦™è°ƒ" (Without verification)
- GOOD: "æ ¹æ®æˆ‘çš„äº†è§£ï¼ŒChanel No. 5 é€šå¸¸åŒ…å«..." (Based on verified knowledge)

Remember: It's better to be vague and honest than to make up specific content that doesn't exist. The search results will be provided to you when needed.
""".strip()

    # ä¼šè¯ IDï¼šå‰ç«¯å¯ä¼ å…¥ï¼›å¦‚æœä¸ºç©ºåˆ™ç”±åç«¯æŒ‰æ—¶é—´æˆ³ç”Ÿæˆä¸€ä¸ªç®€å• ID
    conversation_id = payload.conversation_id or datetime.utcnow().strftime(
        "conv-%Y%m%d%H%M%S%f"
    )

    # å°†å‰ç«¯ä¼ å…¥çš„å¯¹è¯å†å²è½¬æ¢ä¸ºå…¼å®¹ OpenAI çš„ messages ç»“æ„
    history_messages: List[dict] = [
        {"role": msg.role, "content": msg.content} for msg in payload.messages
    ]

    # æ£€æµ‹ç”¨æˆ·æ¶ˆæ¯ä¸­æ˜¯å¦éœ€è¦éªŒè¯çš„å†…å®¹ï¼Œå¹¶æ‰§è¡Œæœç´¢ï¼ˆå¸¦è¶…æ—¶ï¼‰
    # é‡è¦ï¼šæœç´¢å¿…é¡»åœ¨ç”Ÿæˆå›ç­”ä¹‹å‰å®Œæˆï¼Œç¡®ä¿å…ˆæ£€ç´¢éªŒè¯å†å›ç­”
    user_messages = [msg.content for msg in payload.messages if msg.role == "user"]
    last_user_message = user_messages[-1] if user_messages else ""
    search_context = ""
    search_attempted = False  # æ ‡è®°æ˜¯å¦å°è¯•äº†æœç´¢
    search_failed = False  # æ ‡è®°æœç´¢æ˜¯å¦å¤±è´¥
    has_search_results = False  # æ ‡è®°æ˜¯å¦æœ‰å®é™…çš„æœç´¢ç»“æœ
    
    # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ LLM æ„å›¾è¯†åˆ«åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢
    should_search = False
    if last_user_message:
        try:
            import asyncio
            should_search = await asyncio.wait_for(
                detect_intent(last_user_message),
                timeout=5.0  # æ„å›¾è¯†åˆ«è¶…æ—¶æ—¶é—´
            )
            print(f"[æ„å›¾è¯†åˆ«] æœ€ç»ˆåˆ¤æ–­: should_search = {should_search}")
        except asyncio.TimeoutError:
            print("[æ„å›¾è¯†åˆ«] â±ï¸ æ„å›¾è¯†åˆ«è¶…æ—¶ï¼Œå›é€€åˆ°å…³é”®è¯åŒ¹é…")
            # è¶…æ—¶å›é€€åˆ°å…³é”®è¯åŒ¹é…
            should_search = any(kw in last_user_message.lower() for kw in [
                "æœç´¢", "search", "æŸ¥", "æŸ¥æ‰¾", "å¸®æˆ‘æŸ¥", "èƒ½å¦æœç´¢", "ä½ çŸ¥é“",
                "æ­Œè¯", "lyrics", "æ˜¯è°", "å“ªä¸€å¹´", "ä»€ä¹ˆæ—¶å€™"
            ])
        except Exception as e:
            print(f"[æ„å›¾è¯†åˆ«] âŒ æ„å›¾è¯†åˆ«å‡ºé”™: {e}")
            # å‡ºé”™å›é€€åˆ°å…³é”®è¯åŒ¹é…
            should_search = any(kw in last_user_message.lower() for kw in [
                "æœç´¢", "search", "æŸ¥", "æŸ¥æ‰¾", "å¸®æˆ‘æŸ¥", "èƒ½å¦æœç´¢", "ä½ çŸ¥é“",
                "æ­Œè¯", "lyrics", "æ˜¯è°", "å“ªä¸€å¹´", "ä»€ä¹ˆæ—¶å€™"
            ])
    
    print(f"[æœç´¢æ£€æŸ¥] ç”¨æˆ·æ¶ˆæ¯: {last_user_message[:50]}...")
    print(f"[æœç´¢æ£€æŸ¥] tavily_client å¯ç”¨: {tavily_client is not None}")
    print(f"[æœç´¢æ£€æŸ¥] æ„å›¾è¯†åˆ«ç»“æœ: should_search = {should_search}")
    
    # ç¬¬äºŒæ­¥ï¼šå¦‚æœæ„å›¾è¯†åˆ«è¿”å› YESï¼Œå¼ºåˆ¶å¼€å¯æœç´¢
    if tavily_client and last_user_message and should_search:
        search_attempted = True
        print(f"[æœç´¢æ‰§è¡Œ] âœ… å¼€å§‹æ‰§è¡Œæœç´¢...")
        print(f"[æœç´¢æ‰§è¡Œ] tavily_client çŠ¶æ€: {tavily_client is not None}")
        print(f"[æœç´¢æ‰§è¡Œ] ç”¨æˆ·æ¶ˆæ¯: {last_user_message}")
        try:
            import asyncio
            # æœç´¢æœ€å¤šç­‰å¾…15ç§’ï¼Œè¶…æ—¶åˆ™ç»§ç»­å“åº”
            # é‡è¦ï¼šè¿™é‡Œä½¿ç”¨ awaitï¼Œç¡®ä¿æœç´¢å®Œæˆåå†ç»§ç»­
            print(f"[æœç´¢æ‰§è¡Œ] è°ƒç”¨ _perform_searchesï¼ˆä¼ å…¥å®Œæ•´å¯¹è¯å†å²ï¼‰...")
            search_context = await asyncio.wait_for(
                _perform_searches(history_messages),
                timeout=15.0  # å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œç»™æœç´¢æ›´å¤šæ—¶é—´
            )
            print(f"[æœç´¢ç»“æœ] æœç´¢è¿”å›å†…å®¹é•¿åº¦: {len(search_context) if search_context else 0}")
            print(f"[æœç´¢ç»“æœ] æœç´¢è¿”å›å†…å®¹é¢„è§ˆ: {search_context[:200] if search_context else 'None'}...")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…çš„æœç´¢ç»“æœï¼ˆä¸æ˜¯é”™è¯¯æç¤ºï¼‰
            if search_context and search_context.strip():
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å®é™…çš„æœç´¢ç»“æœï¼ˆä¸æ˜¯é”™è¯¯æ¶ˆæ¯ï¼‰
                if "=== è”ç½‘éªŒè¯ç»“æœ" in search_context or "æœç´¢æŸ¥è¯¢:" in search_context:
                    has_search_results = True
                    print(f"[æœç´¢ç»“æœ] âœ… æ‰¾åˆ°æœç´¢ç»“æœ")
                elif "å·²æ‰§è¡Œæœç´¢ä½†æœªæ‰¾åˆ°ç›¸å…³ç»“æœ" in search_context:
                    search_failed = True
                    has_search_results = False
                    print(f"[æœç´¢ç»“æœ] âš ï¸ æœç´¢æ‰§è¡Œäº†ä½†æ²¡æœ‰æ‰¾åˆ°ç»“æœ")
                else:
                    # å¯èƒ½æ˜¯é”™è¯¯æ¶ˆæ¯
                    search_failed = True
                    has_search_results = False
                    print(f"[æœç´¢ç»“æœ] âš ï¸ æœç´¢è¿”å›äº†é”™è¯¯æ¶ˆæ¯")
            else:
                search_failed = True
                has_search_results = False
                print(f"[æœç´¢ç»“æœ] âŒ æœç´¢æ‰§è¡Œäº†ä½†æ²¡æœ‰è¿”å›ç»“æœ")
        except asyncio.TimeoutError:
            print("[æœç´¢ç»“æœ] â±ï¸ æœç´¢è¶…æ—¶")
            search_failed = True
            has_search_results = False
            search_context = "\n\nâš ï¸ æœç´¢è¶…æ—¶ï¼Œæ— æ³•è·å–éªŒè¯ç»“æœã€‚è¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ï¼š'æˆ‘å°è¯•æœç´¢äº†ç›¸å…³ä¿¡æ¯ï¼Œä½†æœç´¢è¶…æ—¶æœªèƒ½è·å–ç»“æœã€‚' ç¦æ­¢ç¼–é€ ç­”æ¡ˆã€‚\n"
        except Exception as e:
            print(f"[æœç´¢ç»“æœ] âŒ æœç´¢è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            search_failed = True
            has_search_results = False
            search_context = ""
    elif should_search and not tavily_client:
        # æ„å›¾è¯†åˆ«è¦æ±‚æœç´¢ä½†æœç´¢æœåŠ¡ä¸å¯ç”¨
        search_attempted = True
        search_failed = True
        has_search_results = False
        print("[æœç´¢ç»“æœ] âŒ æ„å›¾è¯†åˆ«è¦æ±‚æœç´¢ä½†æœç´¢æœåŠ¡ä¸å¯ç”¨ï¼ˆtavily_client æœªåˆå§‹åŒ–ï¼‰")
        search_context = "\n\nâš ï¸ æœç´¢æœåŠ¡å½“å‰ä¸å¯ç”¨ã€‚ä½ å¿…é¡»æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ï¼š'æŠ±æ­‰ï¼Œæœç´¢åŠŸèƒ½æš‚æ—¶æ— æ³•ä½¿ç”¨ã€‚' ç¦æ­¢å£°ç§°å·²æœç´¢æˆ–ç¼–é€ æœç´¢ç»“æœã€‚\n"
    else:
        print(f"[æœç´¢æ£€æŸ¥] è·³è¿‡æœç´¢ï¼ˆæ„å›¾è¯†åˆ«åˆ¤æ–­ä¸éœ€è¦æœç´¢ï¼‰")

    def stream():
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œå¦‚æœæœ‰æœç´¢ç»“æœï¼Œæ·»åŠ åˆ°ç³»ç»Ÿæç¤ºè¯ä¸­
            messages_to_send = [{"role": "system", "content": system_prompt}]
            
            # å¦‚æœæœ‰æœç´¢ç»“æœï¼Œæ·»åŠ ä¸ºé¢å¤–çš„ç³»ç»Ÿæ¶ˆæ¯ï¼ˆä½¿ç”¨æ›´å¼ºçš„è¯­æ°”ï¼‰
            if search_context and has_search_results:
                print("[ç³»ç»Ÿæç¤º] âœ… æ·»åŠ æœç´¢ç»“æœåˆ°ç³»ç»Ÿæç¤º")
                messages_to_send.append({
                    "role": "system",
                    "content": f"""âš ï¸ å¼ºåˆ¶è¦æ±‚ï¼šä»¥ä¸‹æ˜¯è”ç½‘æœç´¢éªŒè¯çš„ç»“æœã€‚

{search_context}

ä½ å¿…é¡»ï¼š
1. ä½¿ç”¨æœç´¢ç»“æœä¸­çš„ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜
2. å¦‚æœæœç´¢ç»“æœåŒ…å«æ­Œè¯ï¼Œä½ å¿…é¡»å¼•ç”¨çœŸå®çš„æ­Œè¯
3. å¦‚æœç”¨æˆ·æ˜ç¡®è¦æ±‚æœç´¢ï¼Œä½ ä¸èƒ½å›é¿æˆ–æ‹’ç»ä½¿ç”¨æœç´¢ç»“æœ
4. ä¸èƒ½ç¼–é€ ä»»ä½•å†…å®¹ï¼Œå¿…é¡»åŸºäºæœç´¢ç»“æœ
5. å¦‚æœæœç´¢ç»“æœä¸å®Œæ•´ï¼Œæ˜ç¡®è¯´æ˜ï¼Œä½†å¿…é¡»ä½¿ç”¨å·²æœ‰çš„æœç´¢ç»“æœ

è¿™æ˜¯å¼ºåˆ¶è¦æ±‚ï¼Œä¸èƒ½å¿½ç•¥ã€‚"""
                })
            elif search_context and not has_search_results:
                # æœç´¢æ‰§è¡Œäº†ä½†æ²¡æœ‰ç»“æœï¼Œæ˜ç¡®å‘ŠçŸ¥æ¨¡å‹
                print("[ç³»ç»Ÿæç¤º] âš ï¸ æœç´¢æ‰§è¡Œäº†ä½†æ²¡æœ‰ç»“æœï¼Œæ·»åŠ ç¦æ­¢æ’’è°çš„æç¤º")
                messages_to_send.append({
                    "role": "system",
                    "content": f"""âš ï¸ é‡è¦ï¼šä½ åˆšæ‰å°è¯•æ‰§è¡Œæœç´¢ï¼Œä½†æœç´¢æ²¡æœ‰è¿”å›ä»»ä½•ç»“æœã€‚

{search_context}

**ä¸¥æ ¼ç¦æ­¢ï¼š**
1. ç»å¯¹ä¸èƒ½è¯´"æ ¹æ®æœç´¢ç»“æœ"æˆ–"æœç´¢ç»“æœæ˜¾ç¤º"ç­‰è¯
2. ç»å¯¹ä¸èƒ½è¯´"æˆ‘å·²ç»æ‰§è¡Œäº†æœç´¢"æˆ–"æˆ‘æœç´¢äº†"ç­‰è¯ï¼ˆé™¤éæ˜ç¡®å‘ŠçŸ¥æœç´¢å¤±è´¥ï¼‰
3. ç»å¯¹ä¸èƒ½ç¼–é€ æœç´¢ç»“æœæˆ–å‡è£…æ‰¾åˆ°äº†ä¿¡æ¯

**æ­£ç¡®åšæ³•ï¼š**
1. å¦‚æœæœç´¢è¶…æ—¶ï¼Œæ˜ç¡®å‘ŠçŸ¥ï¼š"æˆ‘å°è¯•æœç´¢äº†ï¼Œä½†æœç´¢è¶…æ—¶æœªèƒ½è·å–ç»“æœ"
2. å¦‚æœæœç´¢æ²¡æœ‰ç»“æœï¼Œæ˜ç¡®å‘ŠçŸ¥ï¼š"æˆ‘æœç´¢äº†ç›¸å…³ä¿¡æ¯ï¼Œä½†æ²¡æœ‰æ‰¾åˆ°å‡†ç¡®çš„ç­”æ¡ˆ"
3. å¦‚æœæœç´¢æœåŠ¡ä¸å¯ç”¨ï¼Œæ˜ç¡®å‘ŠçŸ¥ï¼š"æŠ±æ­‰ï¼Œæœç´¢åŠŸèƒ½æš‚æ—¶æ— æ³•ä½¿ç”¨"
4. åŸºäºä½ çš„çŸ¥è¯†åº“å›ç­”ï¼Œä½†å¿…é¡»è¯šå®è¯´æ˜è¿™æ˜¯åŸºäºä½ çš„çŸ¥è¯†ï¼Œä¸æ˜¯æœç´¢ç»“æœ

è®°ä½ï¼šè¯šå®æ¯”æ’’è°æ›´é‡è¦ã€‚"""
                })
            elif search_attempted and search_failed:
                # å°è¯•äº†æœç´¢ä½†å¤±è´¥äº†ï¼Œæ˜ç¡®å‘ŠçŸ¥æ¨¡å‹
                messages_to_send.append({
                    "role": "system",
                    "content": f"""âš ï¸ é‡è¦ï¼šä½ åˆšæ‰å°è¯•æ‰§è¡Œæœç´¢ï¼Œä½†æœç´¢å¤±è´¥äº†æˆ–æ²¡æœ‰è¿”å›ç»“æœã€‚

**ä¸¥æ ¼ç¦æ­¢ï¼š**
1. ç»å¯¹ä¸èƒ½è¯´"æˆ‘å·²ç»æ‰§è¡Œäº†æœç´¢"æˆ–"æˆ‘æœç´¢äº†"æˆ–"æˆ‘æŸ¥äº†ç½‘é¡µ"ç­‰ç±»ä¼¼çš„è¯
2. ç»å¯¹ä¸èƒ½è¯´"æœç´¢ç»“æœæ˜¾ç¤º"æˆ–"æ ¹æ®æœç´¢ç»“æœ"ç­‰
3. ç»å¯¹ä¸èƒ½ç¼–é€ æœç´¢ç»“æœæˆ–å‡è£…æ‰¾åˆ°äº†ä¿¡æ¯

**æ­£ç¡®åšæ³•ï¼š**
1. å¦‚æœç”¨æˆ·è¦æ±‚æœç´¢ï¼Œæ˜ç¡®å‘ŠçŸ¥ï¼š"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æ‰§è¡Œæœç´¢"æˆ–"æœç´¢åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨"
2. å¦‚æœæœç´¢è¶…æ—¶ï¼Œæ˜ç¡®å‘ŠçŸ¥ï¼š"æˆ‘å°è¯•æœç´¢äº†ï¼Œä½†æœç´¢è¶…æ—¶æœªèƒ½è·å–ç»“æœ"
3. åŸºäºä½ çš„çŸ¥è¯†åº“å›ç­”ï¼Œä½†å¿…é¡»è¯šå®è¯´æ˜è¿™æ˜¯åŸºäºä½ çš„çŸ¥è¯†ï¼Œä¸æ˜¯æœç´¢ç»“æœ

è®°ä½ï¼šè¯šå®æ¯”æ’’è°æ›´é‡è¦ã€‚"""
                })
            elif should_search and not search_attempted:
                # æ„å›¾è¯†åˆ«è¦æ±‚æœç´¢ä½†æ²¡æœ‰å°è¯•ï¼ˆå¯èƒ½æ˜¯æ£€æµ‹é€»è¾‘é—®é¢˜æˆ–æœç´¢æœåŠ¡ä¸å¯ç”¨ï¼‰
                messages_to_send.append({
                    "role": "system",
                    "content": """âš ï¸ é‡è¦ï¼šç³»ç»Ÿåˆ¤æ–­éœ€è¦æœç´¢ï¼Œä½†æœç´¢åŠŸèƒ½æ²¡æœ‰è¢«è§¦å‘ã€‚

**ä¸¥æ ¼ç¦æ­¢ï¼š**
1. ç»å¯¹ä¸èƒ½è¯´"æˆ‘å·²ç»æ‰§è¡Œäº†æœç´¢"æˆ–"æˆ‘æœç´¢äº†"ç­‰
2. ç»å¯¹ä¸èƒ½ç¼–é€ æœç´¢ç»“æœ

**æ­£ç¡®åšæ³•ï¼š**
æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ï¼š"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æ‰§è¡Œæœç´¢åŠŸèƒ½ã€‚ä½†æˆ‘å¯ä»¥åŸºäºæˆ‘çš„çŸ¥è¯†æ¥å›ç­”ä½ çš„é—®é¢˜ã€‚"

è®°ä½ï¼šè¯šå®æ¯”æ’’è°æ›´é‡è¦ã€‚"""
                })
            
            messages_to_send.extend(history_messages)
            
            completion_stream = client.chat.completions.create(
                model="deepseek-v3.2",
                messages=messages_to_send,
                stream=True,
            )
            collected_chunks: List[str] = []
            try:
                for chunk in completion_stream:
                    if not chunk or not hasattr(chunk, 'choices') or not chunk.choices:
                        continue
                    choice = chunk.choices[0]
                    delta = getattr(choice, "delta", None)
                    if delta and getattr(delta, "content", None):
                        # ç›´æ¥æŠŠå†…å®¹ç‰‡æ®µå†™å›ç»™å‰ç«¯ï¼Œç”±å‰ç«¯ç´¯ç§¯
                        collected_chunks.append(delta.content)
                        yield delta.content
            except Exception as stream_error:
                print(f"[æµå¼å“åº”] æµå¼è¯»å–é”™è¯¯: {stream_error}")
                import traceback
                traceback.print_exc()
                # å¦‚æœæµå¼è¯»å–å‡ºé”™ï¼Œè‡³å°‘è¿”å›å·²æ”¶é›†çš„å†…å®¹
                if collected_chunks:
                    yield "".join(collected_chunks)
                # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©å¤–å±‚ catch å¤„ç†
                raise

            # æµå¼ç»“æŸåï¼Œå°†å®Œæ•´å¯¹è¯ä¿å­˜åˆ°ä¼šè¯å­˜å‚¨ä¸­
            assistant_text = "".join(collected_chunks)
            if assistant_text:
                stored_messages = [
                    {"role": m["role"], "content": m["content"]}
                    for m in history_messages
                ]
                stored_messages.append(
                    {"role": "assistant", "content": assistant_text}
                )
                
                # ä¿å­˜ç”¨æˆ·åå­—ï¼šä¼˜å…ˆä½¿ç”¨è¯·æ±‚ä¸­çš„åå­—ï¼Œå…¶æ¬¡ä»å¯¹è¯ä¸­æå–
                final_user_name = user_name or _extract_user_name(stored_messages, None)
                _save_conversation(conversation_id, stored_messages, final_user_name)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆä¹‹å‰çš„å»¶è¿Ÿä»»åŠ¡ï¼ˆå¦‚æœç”¨æˆ·ç»§ç»­å¯¹è¯ï¼‰
                # ç„¶åå¯åŠ¨æ–°çš„5åˆ†é’Ÿå»¶è¿Ÿæ£€æŸ¥ä»»åŠ¡
                background_tasks.add_task(_check_and_generate_memo, conversation_id, stored_messages, final_user_name, payload.locale)
        except Exception as e:
            # é‡åˆ°å¼‚å¸¸æ—¶ç«‹å³ä¸­æ­¢ï¼Œå¹¶è®©å‰ç«¯èµ°å…œåº•é€»è¾‘
            raise HTTPException(status_code=500, detail=f"LLM æµå¼è°ƒç”¨å¤±è´¥: {e}") from e

    return StreamingResponse(stream(), media_type="text/plain; charset=utf-8")


async def _check_and_generate_memo(conversation_id: str, messages: List[dict], user_name: Optional[str], locale: str):
    """æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‰‹æœ­ï¼ˆ5åˆ†é’Ÿæ— å“åº”åç”Ÿæˆï¼‰"""
    import asyncio
    # ç­‰å¾…5åˆ†é’Ÿ
    await asyncio.sleep(300)  # 300ç§’ = 5åˆ†é’Ÿ
    
    try:
        path = _conversation_path(conversation_id)
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯çš„æ—¶é—´
        last_message_time_str = data.get("last_message_time")
        if not last_message_time_str:
            return
        
        try:
            last_message_time = datetime.fromisoformat(last_message_time_str.replace("Z", "+00:00"))
            now = datetime.utcnow().replace(tzinfo=last_message_time.tzinfo)
            time_diff = (now - last_message_time).total_seconds()
            
            # å¦‚æœè·ç¦»æœ€åä¸€æ¡æ¶ˆæ¯å·²ç»è¶…è¿‡5åˆ†é’Ÿï¼Œä¸”æ²¡æœ‰æ–°æ¶ˆæ¯ï¼Œç”Ÿæˆæ‰‹æœ­
            if time_diff >= 300:  # 5åˆ†é’Ÿ = 300ç§’
                current_message_count = len(data.get("messages", []))
                last_message_count = data.get("memo_last_message_count", 0)
                
                # å¦‚æœæ¶ˆæ¯æ•°é‡æ²¡æœ‰å¢åŠ ï¼Œè¯´æ˜ç¡®å®æš‚åœäº†ï¼Œç”Ÿæˆæ‰‹æœ­
                if current_message_count > last_message_count:
                    await _generate_and_save_memo(conversation_id, data.get("messages", []), user_name, locale, data)
        except Exception as e:
            print(f"æ£€æŸ¥æ‰‹æœ­ç”Ÿæˆæ—¶é—´å¤±è´¥: {e}")
    except Exception as e:
        print(f"æ£€æŸ¥æ‰‹æœ­ç”Ÿæˆå¤±è´¥: {e}")


async def _generate_and_save_memo(conversation_id: str, messages: List[dict], user_name: Optional[str], locale: str, data: Optional[dict] = None):
    """å¼‚æ­¥ç”Ÿæˆå¹¶ä¿å­˜æ‰‹æœ­ï¼ˆæ”¯æŒè¿½åŠ æ›´æ–°ï¼ŒåŸºäºä¼šè¯æ®µï¼‰"""
    try:
        path = _conversation_path(conversation_id)
        if data is None:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
        
        existing_memo = data.get("memo")
        last_message_count = data.get("memo_last_message_count", 0)
        current_message_count = len(messages)
        
        # å¦‚æœæ¶ˆæ¯æ•°é‡æ²¡æœ‰å¢åŠ ï¼Œè¯´æ˜æ²¡æœ‰æ–°å¯¹è¯ï¼Œä¸éœ€è¦æ›´æ–°æ‰‹æœ­
        if current_message_count <= last_message_count:
            return
        
        # è·å–æ–°å¢çš„æ¶ˆæ¯ï¼ˆä»ä¸Šæ¬¡ç”Ÿæˆæ‰‹æœ­ä¹‹åçš„æ¶ˆæ¯ï¼‰
        new_messages = messages[last_message_count:] if last_message_count > 0 else messages
        
        # å¦‚æœè¿™æ˜¯ç¬¬ä¸€æ¬¡ç”Ÿæˆæ‰‹æœ­ï¼Œç”Ÿæˆå®Œæ•´çš„æ‰‹æœ­
        if not existing_memo or last_message_count == 0:
            conversation_data = {
                "id": conversation_id,
                "messages": messages,
                "user_name": user_name,
                "created_at": data.get("created_at", datetime.utcnow().isoformat() + "Z"),
            }
            memo = await _generate_memo_summary(conversation_data, locale, is_update=False)
            data["memo"] = memo
        else:
            # å¦‚æœæœ‰å·²æœ‰æ‰‹æœ­ï¼Œç”Ÿæˆæ–°å¢å¯¹è¯çš„æ‘˜è¦å¹¶è¿½åŠ 
            conversation_data = {
                "id": conversation_id,
                "messages": new_messages,
                "user_name": user_name,
                "created_at": data.get("updated_at", datetime.utcnow().isoformat() + "Z"),
            }
            new_memo_section = await _generate_memo_summary(conversation_data, locale, is_update=True)
            # è¿½åŠ æ–°æ‰‹æœ­å†…å®¹åˆ°å·²æœ‰æ‰‹æœ­
            if locale == "zh":
                data["memo"] = f"{existing_memo}\n\n{new_memo_section}"
            else:
                data["memo"] = f"{existing_memo}\n\n{new_memo_section}"
        
        # æ›´æ–°æ¶ˆæ¯æ•°é‡è®°å½•å’Œæ‰‹æœ­ç”Ÿæˆæ—¶é—´
        data["memo_last_message_count"] = current_message_count
        data["last_memo_time"] = datetime.utcnow().isoformat() + "Z"
        
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        # æ‰‹æœ­ç”Ÿæˆå¤±è´¥ä¸å½±å“ä¸»æµç¨‹
        print(f"æ‰‹æœ­ç”Ÿæˆå¤±è´¥: {e}")


@app.get("/api/health")
async def health_check() -> dict:
    """ç®€å•å¥åº·æ£€æŸ¥ï¼Œä¾¿äºè°ƒè¯•è¿é€šæ€§ã€‚"""
    return {"status": "ok", "service": "scent-alchemist-chat-api"}


@app.get("/api/conversations")
async def list_conversations(locale: str = "zh") -> List[dict]:
    """åˆ—å‡ºå·²ä¿å­˜çš„ä¼šè¯ï¼Œè¿”å›åŸå§‹æ ‡é¢˜å’Œæ‰‹æœ­æ‘˜è¦ã€‚"""
    items: List[dict] = []
    for filename in os.listdir(CONVERSATIONS_DIR):
        if not filename.endswith(".json"):
            continue
        path = os.path.join(CONVERSATIONS_DIR, filename)
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception:
            continue
        
        messages = data.get("messages") or []
        # ä½¿ç”¨åŸå§‹å¯¹è¯çš„ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºæ ‡é¢˜
        first_user = next(
            (m.get("content", "") for m in messages if m.get("role") == "user"), ""
        )
        title = first_user[:40] if first_user else (locale == "zh" and "æœªå‘½åä¼šè¯" or "Untitled")
        
        items.append(
            {
                "id": data.get("id"),
                "created_at": data.get("created_at"),
                "updated_at": data.get("updated_at"),
                "title": title,
                "memo": data.get("memo"),  # å¯é€‰çš„æ‰‹æœ­æ‘˜è¦
            }
        )
    # æŒ‰æ›´æ–°æ—¶é—´å€’åº
    items.sort(key=lambda x: x.get("updated_at") or "", reverse=True)
    return items


@app.get("/api/conversations/{conversation_id}")
async def get_conversation(conversation_id: str, locale: str = "zh") -> dict:
    """è·å–å•ä¸ªä¼šè¯çš„å®Œæ•´å†…å®¹ï¼Œå¦‚æœè¿˜æ²¡æœ‰æ‰‹æœ­åˆ™ç”Ÿæˆã€‚"""
    data = _load_conversation(conversation_id)
    
    # å¦‚æœè¿˜æ²¡æœ‰æ‰‹æœ­æ‘˜è¦ï¼Œç”Ÿæˆä¸€ä¸ª
    if not data.get("memo"):
        memo = await _generate_memo_summary(data, locale)
        data["memo"] = memo
        # ä¿å­˜åˆ°æ–‡ä»¶
        path = _conversation_path(conversation_id)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    return data


class DrawBottleRequest(BaseModel):
    recipe_name: str
    scent_keywords: str
    locale: str = "zh"  # ç”¨äºç”Ÿæˆæè¿°


@app.post("/api/draw_bottle")
async def draw_bottle(payload: DrawBottleRequest):
    """ç”Ÿæˆé¦™æ°´ç“¶è§†è§‰è®¾è®¡
    
    æµç¨‹ï¼š
    1. ä½¿ç”¨ DeepSeek å°†é¦™æ°´åç§°å’Œé¦™è°ƒå…³é”®è¯è½¬åŒ–ä¸ºè‹±æ–‡ Stable Diffusion Prompt
    2. è°ƒç”¨ç™¾åº¦åƒå¸† FLUX.1-schnell æ¨¡å‹ç”Ÿæˆå›¾ç‰‡
    """
    # Step 1: The Translator & Botanical Alchemist - ä½¿ç”¨ DeepSeek ç¿»è¯‘å¹¶ç”Ÿæˆå¤å¤æ¤ç‰©ç‚¼é‡‘æœ¯é£æ ¼çš„ç»˜ç”»æç¤ºè¯
    prompt_template = f"""You are a visual prompt engineer for FLUX. Your job is to translate user inputs into a precise ENGLISH visual description in "Vintage Botanical Alchemist" style.

Input:
Name: {payload.recipe_name} (May be Chinese)
Scent: {payload.scent_keywords} (May be Chinese)

Your Task:
1. Translate the Name to a poetic English name (e.g., "ç ´æ™“" -> "Daybreak", "ç™½é‡‘ç¼®" -> "White Kintsugi").
2. Translate the Scent description to English visual keywords focusing on botanical and atmospheric elements.
3. Construct the final prompt strictly following this template:

Final Prompt Template: "A high-quality vintage botanical illustration of a perfume bottle. The bottle is labeled '{{Translated_English_Name}}' in elegant calligraphy. The bottle shape is [shape_based_on_scent]. Surrounding the bottle are hand-drawn sketches of [specific_ingredients] and [atmospheric_elements] (e.g., floating leaves, water drops, sunlight, morning dew, dappled light, smoke wisps, old ink stains). Texture: textured beige paper, pencil lines with soft watercolor washes. Art style: Pierre-Joseph RedoutÃ©, elegant, organic, hyper-detailed."

CRITICAL RULES:
- The label text MUST be in English ONLY. NO Chinese characters allowed.
- FORBIDDEN: Do NOT use "molecular structure", "chemical formulas", "benzene rings", "scientific diagrams", or any chemistry-related terms.
- REQUIRED: Focus on "Botanical Illustration" and "Atmospheric Elements".
  * If scent mentions "Rose", use "detailed pencil sketch of dried rose petals".
  * Add atmospheric elements like "morning dew drops", "dappled sunlight", "smoke wisps", "old ink stains", "floating leaves".
- Maximum 512 characters total.
- Output ONLY the final prompt text with all placeholders filled in, nothing else.
- Do NOT include any explanation or additional text."""

    try:
        # è°ƒç”¨ DeepSeek ç”Ÿæˆæç¤ºè¯
        completion = client.chat.completions.create(
            model="deepseek-v3.2",
            messages=[
                {"role": "system", "content": "You are a visual prompt engineer for FLUX specializing in Vintage Botanical Alchemist style. You create prompts focused on botanical illustrations and atmospheric elements. NEVER use chemistry-related terms like molecular structures or chemical formulas. Do NOT include any signatures or text in the image (signatures will be added separately). Always output only the final prompt text, no explanations."},
                {"role": "user", "content": prompt_template}
            ],
            temperature=0.7,
            max_tokens=300,
        )
        generated_prompt = completion.choices[0].message.content.strip()
        
        # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°å’Œå¼•å·
        if "```" in generated_prompt:
            # ç§»é™¤ä»£ç å—æ ‡è®°
            if "```" in generated_prompt:
                lines = generated_prompt.split("\n")
                generated_prompt = "\n".join([line for line in lines if not line.strip().startswith("```")])
                generated_prompt = generated_prompt.strip()
        
        # ç§»é™¤å¯èƒ½çš„å¼•å·åŒ…è£…
        if generated_prompt.startswith('"') and generated_prompt.endswith('"'):
            generated_prompt = generated_prompt[1:-1]
        elif generated_prompt.startswith("'") and generated_prompt.endswith("'"):
            generated_prompt = generated_prompt[1:-1]
        
        # ç§»é™¤ä»»ä½•ä¸­æ–‡å­—ç¬¦å’ŒåŒ–å­¦ç›¸å…³è¯æ±‡ï¼ˆåŒé‡ä¿é™©ï¼‰
        import re
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        chinese_pattern = re.compile(r'[\u4e00-\u9fff]+')
        # åŒ–å­¦ç›¸å…³å…³é”®è¯åˆ—è¡¨
        chemistry_keywords = [
            "molecular structure", "chemical formula", "benzene ring", 
            "scientific diagram", "C8H8O3", "C10H12O2", "molecule",
            "chemical", "formula", "structure diagram", "benzene",
            "molecular", "scientific sketch"
        ]
        
        if chinese_pattern.search(generated_prompt):
            # å¦‚æœåŒ…å«ä¸­æ–‡ï¼Œå°è¯•æå–è‹±æ–‡éƒ¨åˆ†
            words = generated_prompt.split()
            english_words = []
            for word in words:
                if not chinese_pattern.search(word):
                    english_words.append(word)
                else:
                    break
            if english_words:
                generated_prompt = " ".join(english_words)
            else:
                # å¦‚æœå…¨æ˜¯ä¸­æ–‡ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼ˆä½¿ç”¨æ¤ç‰©æ’ç”»é£æ ¼ï¼‰
                generated_prompt = "A high-quality vintage botanical illustration of a perfume bottle in elegant calligraphy. Hand-drawn sketches of botanical elements and atmospheric details. Textured beige paper, pencil lines with soft watercolor washes."
        
        # ç§»é™¤åŒ–å­¦ç›¸å…³è¯æ±‡ï¼Œæ›¿æ¢ä¸ºæ¤ç‰©æ’ç”»ç›¸å…³è¯æ±‡
        for keyword in chemistry_keywords:
            if keyword.lower() in generated_prompt.lower():
                # æ›¿æ¢ä¸ºæ¤ç‰©æ’ç”»ç›¸å…³è¯æ±‡
                generated_prompt = re.sub(
                    re.escape(keyword), 
                    "botanical sketches", 
                    generated_prompt, 
                    flags=re.IGNORECASE
                )
        
        # ç§»é™¤ä»»ä½•ç­¾åç›¸å…³çš„æ–‡æœ¬ï¼ˆç­¾åå°†åœ¨å‰ç«¯æ˜¾ç¤ºï¼‰
        generated_prompt = re.sub(
            r"signed\s+['\"]?le\s+nez['\"]?\s+in\s+the\s+corner\.?", 
            "", 
            generated_prompt, 
            flags=re.IGNORECASE
        )
        generated_prompt = re.sub(
            r"signed\s+['\"].*?['\"]", 
            "", 
            generated_prompt, 
            flags=re.IGNORECASE
        )
        generated_prompt = generated_prompt.strip().rstrip(".,")
        if generated_prompt and not generated_prompt.endswith("."):
            generated_prompt += "."
        
        # ç¡®ä¿æç¤ºè¯ä¸è¶…è¿‡ 512 å­—ç¬¦ï¼ˆFLUX.1-schnell é™åˆ¶ï¼‰
        if len(generated_prompt) > 512:
            # å°è¯•åœ¨å¥å·å¤„æˆªæ–­
            last_period = generated_prompt[:512].rfind(".")
            if last_period > 400:  # è‡³å°‘ä¿ç•™400å­—ç¬¦
                generated_prompt = generated_prompt[:last_period + 1]
            else:
                generated_prompt = generated_prompt[:509] + "..."
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to generate prompt: {str(e)}"
        )
    
    # Step 2: The Artist - è°ƒç”¨ç™¾åº¦åƒå¸†å›¾åƒç”Ÿæˆ API
    IMAGE_MODEL_ID = os.getenv("IMAGE_MODEL_ID", "flux.1-schnell")
    QIANFAN_API_URL = "https://qianfan.baidubce.com/v2/images/generations"
    
    # è·å– API Keyï¼ˆå¤ç”¨ OPENAI_API_KEYï¼Œå³ bce-v3 å¼€å¤´çš„ keyï¼‰
    api_key = OPENAI_API_KEY
    if not api_key:
        raise HTTPException(
            status_code=500,
            detail="OPENAI_API_KEY not configured"
        )
    
    # æ„å»ºè¯·æ±‚å¤´
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }
    
    # æ„å»ºè¯·æ±‚ä½“
    request_body = {
        "model": IMAGE_MODEL_ID,
        "prompt": generated_prompt,
        "size": "1024x1024",
        "n": 1,
    }
    
    try:
        # å‘é€ POST è¯·æ±‚åˆ°ç™¾åº¦åƒå¸†
        response = requests.post(
            QIANFAN_API_URL,
            headers=headers,
            json=request_body,
            timeout=60,  # å›¾åƒç”Ÿæˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
        )
        
        if response.status_code != 200:
            error_detail = response.text
            try:
                error_json = response.json()
                error_detail = error_json.get("error", {}).get("message", error_detail)
            except:
                pass
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Qianfan API error: {error_detail}"
            )
        
        result = response.json()
        
        # æå–å›¾ç‰‡ URL
        if "data" in result and len(result["data"]) > 0:
            image_url = result["data"][0].get("url")
            if not image_url:
                raise HTTPException(
                    status_code=500,
                    detail="No image URL in response"
                )
            
            # ç”Ÿæˆé…æ–¹æè¿°å’Œè§£æå‰ä¸­åè°ƒï¼ˆä½¿ç”¨ LLMï¼‰
            is_zh = payload.locale == "zh"
            
            # 1. ç”Ÿæˆè¯—æ„æè¿°
            if is_zh:
                description_prompt = f"""æ ¹æ®é¦™æ°´åã€Œ{payload.recipe_name}ã€å’Œé¦™è°ƒå…³é”®è¯ã€Œ{payload.scent_keywords}ã€ï¼Œåˆ›ä½œä¸€å¥è¯—æ„æè¿°ï¼ˆåŒ…æ‹¬æ ‡ç‚¹ç¬¦å·åœ¨å†…ï¼Œä¸¥æ ¼ä¸è¶…è¿‡40å­—ï¼‰ã€‚

è¦æ±‚ï¼š
- ä¸è¦ç›´æ¥æè¿°é¦™æ–™æˆåˆ†æœ¬èº«ï¼ˆå¦‚"é‡‘å±é†›å…‰"ã€"ç«ç‘°ä½“ä¹³"ç­‰ï¼‰
- æè¿°ä¸€ç§ç”Ÿæ´»ã€ä¸€ç§æ°”æ¯ã€ä¸€ç§æ„Ÿè§‰ï¼Œæ˜¯è¿™ç§é¦™æ–™å¸¦ç»™äººçš„è®°å¿†å’Œæƒ…æ„Ÿ
- æ„å»ºä¸€ä¸ªåœºæ™¯ï¼Œæè¿°åœºæ™¯å¸¦ç»™äººçš„æ„Ÿè§‰å’Œæ°›å›´ï¼Œè€Œä¸æ˜¯åœºæ™¯ä¸­çš„é¦™æ–™å‘³
- è¯­è¨€è¦è¯—æ„ã€æŠ½è±¡ã€å¯Œæœ‰æƒ³è±¡åŠ›ï¼Œå”¤èµ·è¯»è€…çš„æƒ…æ„Ÿå…±é¸£
- ä¾‹å¦‚ï¼šä¸æ˜¯"é‡‘å±é†›å…‰ç©¿é€è¢«çª"ï¼Œè€Œæ˜¯"æ™¨å…‰é€è¿‡çª—å¸˜ï¼Œå”¤é†’æ²‰ç¡çš„æ¢¦å¢ƒ"è¿™æ ·çš„æ„Ÿè§‰

è¯·åˆ›ä½œä¸€å¥ä¸è¶…è¿‡40å­—çš„è¯—æ„æè¿°ã€‚"""
            else:
                description_prompt = f"""Based on the perfume name '{payload.recipe_name}' and scent notes '{payload.scent_keywords}', write a brief, poetic description in English (including punctuation, strictly no more than 40 characters).

Requirements:
- Do not directly describe the scent notes themselves (e.g., "metallic aldehyde", "rose body lotion")
- Describe a way of life, an atmosphere, a feelingâ€”the memories and emotions that these scents evoke
- Build a scene and describe the feelings and atmosphere it brings, not the scent notes in the scene
- Language should be poetic, abstract, and imaginative, evoking emotional resonance
- For example: not "metallic aldehyde light penetrates the covers", but something like "morning light through curtains, awakening sleeping dreams"

Write a poetic description of no more than 40 characters."""
            
            # 2. è§£æå‰ä¸­åè°ƒï¼ˆå¿…é¡»ä½¿ç”¨å…·ä½“çš„é¦™å‘³ä¸“ä¸šè¯è¯­ï¼‰
            if is_zh:
                notes_prompt = f"""æ ¹æ®é¦™è°ƒå…³é”®è¯ã€Œ{payload.scent_keywords}ã€ï¼Œå°†å…¶åˆ†ç±»ä¸ºå‰è°ƒã€ä¸­è°ƒã€åè°ƒã€‚

CRITICAL REQUIREMENTS:
- å¿…é¡»ä½¿ç”¨å…·ä½“çš„é¦™å‘³ä¸“ä¸šè¯è¯­ï¼Œä¾‹å¦‚ï¼šç«ç‘°ã€è—çº¢èŠ±ã€æ´‹ç”˜èŠã€ç»¿å¶ã€æª€æœ¨ã€é¦™çš‚ã€èŒ‰è‰ã€è–°è¡£è‰ã€é›ªæ¾ã€ç¥ç€ã€éºé¦™ã€é¦™è‰ã€æŸ æª¬ã€æ©™èŠ±ã€å¹¿è—¿é¦™ã€ä¾å…°ã€é¸¢å°¾ã€ç´«ç½—å…°ã€ç™½èŠ±ã€æœ¨è´¨ã€æ ‘è„‚ã€é¦™æ–™ç­‰ã€‚
- ç»å¯¹ç¦æ­¢ä½¿ç”¨æŠ½è±¡è¯è¯­ï¼Œä¾‹å¦‚ï¼šå†…æ•›å«è“„ã€å±‚æ¬¡ä¸°å¯Œã€ä¸œæ–¹ç¾å­¦ã€å¹³å’Œæ·±é‚ƒã€ç¦…æ„æ„å¢ƒã€ä¼˜é›…ã€ç¥ç§˜ã€æ¸©æš–ã€æ¸…æ–°ç­‰ã€‚
- å¦‚æœå…³é”®è¯å¯ä»¥æ˜ç¡®åˆ†ä¸ºå‰ä¸­åè°ƒï¼Œè¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›JSONï¼š
  {{"has_notes": true, "top": "å…·ä½“é¦™å‘³è¯è¯­1, å…·ä½“é¦™å‘³è¯è¯­2", "middle": "å…·ä½“é¦™å‘³è¯è¯­1, å…·ä½“é¦™å‘³è¯è¯­2", "base": "å…·ä½“é¦™å‘³è¯è¯­1, å…·ä½“é¦™å‘³è¯è¯­2"}}
- å¦‚æœå…³é”®è¯æ˜¯å•ä¸€è¡¨è¾¾ï¼ˆæ— æ³•åŒºåˆ†å‰ä¸­åè°ƒï¼‰ï¼Œè¯·è¿”å›ï¼š
  {{"has_notes": false, "single": "å…·ä½“é¦™å‘³è¯è¯­1, å…·ä½“é¦™å‘³è¯è¯­2"}}
- åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Š
- å¦‚æœåŸå…³é”®è¯åŒ…å«æŠ½è±¡è¯è¯­ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­å¹¶æ›¿æ¢ä¸ºå…·ä½“çš„é¦™å‘³ä¸“ä¸šè¯è¯­"""
            else:
                notes_prompt = f"""Based on the scent keywords '{payload.scent_keywords}', classify them into top, middle, and base notes.

CRITICAL REQUIREMENTS:
- MUST use specific perfume note terminology, e.g.: rose, saffron, chamomile, green leaves, sandalwood, soap, jasmine, lavender, cedar, amber, musk, vanilla, lemon, neroli, patchouli, ylang-ylang, iris, violet, white flowers, woody, resin, spices, etc.
- ABSOLUTELY FORBIDDEN: abstract terms like "subtle", "rich layers", "elegant", "mysterious", "warm", "fresh", "oriental aesthetics", "peaceful", "profound", "zen", etc.
- If keywords can be clearly divided into top/middle/base notes, return JSON in this format:
  {{"has_notes": true, "top": "specific note 1, specific note 2", "middle": "specific note 1, specific note 2", "base": "specific note 1, specific note 2"}}
- If keywords represent a single expression (cannot be divided), return:
  {{"has_notes": false, "single": "specific note 1, specific note 2"}}
- Return ONLY JSON, no explanations
- If original keywords contain abstract terms, infer and replace them with specific perfume note terminology"""
            
            try:
                # ç”Ÿæˆæè¿°
                desc_completion = client.chat.completions.create(
                    model="deepseek-v3.2",
                    messages=[
                        {"role": "system", "content": f"You are a poetic writer specializing in perfume descriptions. Your descriptions focus on emotions, memories, and atmospheres rather than scent notes themselves. Write concise, evocative descriptions that capture the feeling and mood. {'ä¸¥æ ¼æ§åˆ¶åœ¨40å­—ä»¥å†…ï¼ˆåŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰' if is_zh else 'Strictly no more than 40 characters (including punctuation)'}."},
                        {"role": "user", "content": description_prompt}
                    ],
                    temperature=0.9,
                    max_tokens=100,
                )
                description = desc_completion.choices[0].message.content.strip()
                # å¦‚æœè¶…è¿‡40å­—ï¼ˆä¸­æ–‡ï¼‰æˆ–40å­—ç¬¦ï¼ˆè‹±æ–‡ï¼‰ï¼Œæˆªæ–­
                if is_zh:
                    if len(description) > 40:
                        for punct in ['ã€‚', 'ï¼Œ', 'ã€', 'ï¼›', 'ï¼š', '.', ',', ';', ':']:
                            idx = description[:40].rfind(punct)
                            if idx > 20:
                                description = description[:idx+1]
                                break
                        else:
                            description = description[:40]
                else:
                    if len(description) > 40:
                        for punct in ['.', ',', ';', ':', '!', '?']:
                            idx = description[:40].rfind(punct)
                            if idx > 20:
                                description = description[:idx+1]
                                break
                        else:
                            description = description[:40]
                
                # è§£æå‰ä¸­åè°ƒï¼ˆå¿…é¡»ä½¿ç”¨å…·ä½“çš„é¦™å‘³ä¸“ä¸šè¯è¯­ï¼‰
                notes_completion = client.chat.completions.create(
                    model="deepseek-v3.2",
                    messages=[
                        {"role": "system", "content": "You are a perfume expert. Analyze scent keywords and classify them into perfume notes. CRITICAL: You MUST use specific perfume note terminology (e.g., rose, jasmine, sandalwood, musk, vanilla, citrus, etc.). ABSOLUTELY FORBIDDEN: abstract terms like 'subtle', 'elegant', 'mysterious', 'rich layers', 'oriental aesthetics', etc. Always return valid JSON only."},
                        {"role": "user", "content": notes_prompt}
                    ],
                    temperature=0.5,
                    max_tokens=200,
                )
                notes_text = notes_completion.choices[0].message.content.strip()
                
                # è§£æJSON
                import json
                if "```json" in notes_text:
                    notes_text = notes_text.split("```json")[1].split("```")[0].strip()
                elif "```" in notes_text:
                    notes_text = notes_text.split("```")[1].split("```")[0].strip()
                
                notes_data = json.loads(notes_text)
                
            except Exception as e:
                # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                description = payload.scent_keywords
                if is_zh and len(description) > 40:
                    description = description[:40]
                elif not is_zh and len(description) > 40:
                    description = description[:40]
                # é»˜è®¤ä½¿ç”¨å•ä¸€è¡¨è¾¾
                notes_data = {
                    "has_notes": False,
                    "single": payload.scent_keywords
                }
            
            # ä¿å­˜é…æ–¹
            import time
            recipe_id = f"recipe-{int(time.time() * 1000)}-{''.join([str(ord(c)) for c in payload.recipe_name[:3]])}"
            recipe_data = {
                "id": recipe_id,
                "name": payload.recipe_name,
                "keywords": payload.scent_keywords,
                "description": description,
                "image_url": image_url,
                "created_at": datetime.now().isoformat(),
                "locale": payload.locale,
                "notes": notes_data,  # æ·»åŠ å‰ä¸­åè°ƒä¿¡æ¯
            }
            _save_recipe(recipe_id, recipe_data)
            
            return {
                "image_url": image_url,
                "recipe_id": recipe_id,
                "notes": notes_data  # è¿”å›å‰ä¸­åè°ƒä¿¡æ¯
            }
        else:
            raise HTTPException(
                status_code=500,
                detail="Invalid response format from Qianfan API"
            )
            
    except requests.exceptions.Timeout:
        raise HTTPException(
            status_code=504,
            detail="Image generation timeout"
        )
    except requests.exceptions.RequestException as e:
        raise HTTPException(
            status_code=500,
            detail=f"Request to Qianfan API failed: {str(e)}"
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Unexpected error: {str(e)}"
        )


class ExtractRecipeRequest(BaseModel):
    text: str
    locale: str = "zh"  # é»˜è®¤ä¸­æ–‡


@app.post("/api/extract_recipe")
async def extract_recipe(payload: ExtractRecipeRequest):
    """ä»èŠå¤©è®°å½•æˆ–æ–‡æœ¬ä¸­æå–é¦™æ°´åå’Œå…³é”®è¯
    
    ä½¿ç”¨ DeepSeek LLM ä»éç»“æ„åŒ–æ–‡æœ¬ä¸­æå–ï¼š
    - name: é¦™æ°´åç§°ï¼ˆæ ¹æ® locale è¿”å›å¯¹åº”è¯­è¨€ï¼‰
    - keywords: è‹±æ–‡è§†è§‰å…³é”®è¯ï¼ˆ3-5ä¸ªï¼Œå§‹ç»ˆä¸ºè‹±æ–‡ï¼Œç”¨äºå›¾åƒç”Ÿæˆï¼‰
    """
    is_zh = payload.locale == "zh"
    
    name_instruction = (
        "æå–é¦™æ°´åç§°ï¼ˆå¦‚æœæåˆ°ï¼‰æˆ–æ ¹æ®å†…å®¹å»ºè®®ä¸€ä¸ªå¯Œæœ‰è¯—æ„çš„ä¸­æ–‡åç§°"
        if is_zh
        else "Extract the perfume name (if mentioned) or suggest a poetic name in English based on the content"
    )
    
    keywords_instruction = (
        "æå–3-5ä¸ªä¸­æ–‡è§†è§‰å…³é”®è¯ï¼Œæè¿°é¦™è°ƒçš„æ°›å›´ã€æƒ…ç»ªæˆ–è§†è§‰å…ƒç´ ï¼ˆä¾‹å¦‚ï¼šæ½®æ¹¿ã€è‹”è—“ã€å¢¨æ°´ã€æ—§ä¹¦ã€ç”µå½±æ„Ÿå…‰çº¿ï¼‰ã€‚è¿™äº›å…³é”®è¯å°†ç”¨äºå›¾åƒç”Ÿæˆï¼Œè¯·ç¡®ä¿å®ƒä»¬èƒ½å¤Ÿå‡†ç¡®ä¼ è¾¾é¦™è°ƒçš„æ„Ÿè§‰ã€‚"
        if is_zh
        else "Extract 3-5 visual keywords in English that describe the mood, atmosphere, or visual elements. Keywords should be suitable for image generation (e.g., 'damp', 'moss', 'ink', 'old books', 'cinematic lighting')"
    )
    
    extract_prompt = f"""Extract the perfume name and 3-5 visual keywords from the following text.

Text:
{payload.text}

Requirements:
- {name_instruction}
- {keywords_instruction}
- Return ONLY valid JSON in this exact format:
{{
  "name": "Perfume Name",
  "keywords": "keyword1, keyword2, keyword3, keyword4, keyword5"
}}

Do not include any explanation or additional text, only the JSON object."""

    try:
        completion = client.chat.completions.create(
            model="deepseek-v3.2",
            messages=[
                {"role": "system", "content": "You are a helpful assistant that extracts structured information from text. Always return valid JSON only."},
                {"role": "user", "content": extract_prompt}
            ],
            temperature=0.5,
            max_tokens=200,
        )
        
        response_text = completion.choices[0].message.content.strip()
        
        # å°è¯•è§£æ JSONï¼ˆå¯èƒ½åŒ…å«ä»£ç å—ï¼‰
        import json
        # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        if "```json" in response_text:
            response_text = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            response_text = response_text.split("```")[1].split("```")[0].strip()
        
        result = json.loads(response_text)
        
        # éªŒè¯å¹¶è¿”å›
        return {
            "name": result.get("name", ""),
            "keywords": result.get("keywords", "")
        }
        
    except json.JSONDecodeError as e:
        # å¦‚æœ JSON è§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–
        raise HTTPException(
            status_code=500,
            detail=f"Failed to parse LLM response as JSON: {str(e)}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to extract recipe: {str(e)}"
        )


@app.get("/api/recipes")
async def list_recipes(locale: str = "zh") -> List[dict]:
    """åˆ—å‡ºå·²ä¿å­˜çš„é¦™æ°´é…æ–¹"""
    items: List[dict] = []
    for filename in os.listdir(RECIPES_DIR):
        if not filename.endswith(".json"):
            continue
        path = os.path.join(RECIPES_DIR, filename)
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                items.append({
                    "id": data.get("id", filename.replace(".json", "")),
                    "name": data.get("name", ""),
                    "description": data.get("description", ""),
                    "created_at": data.get("created_at", ""),
                })
        except Exception as e:
            print(f"Error loading recipe {filename}: {e}")
            continue
    
    # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
    items.sort(key=lambda x: x.get("created_at", ""), reverse=True)
    return items


@app.get("/api/recipes/{recipe_id}")
async def get_recipe(recipe_id: str) -> dict:
    """è·å–å•ä¸ªé…æ–¹çš„å®Œæ•´å†…å®¹"""
    return _load_recipe(recipe_id)


class ScentRequest(BaseModel):
    name: str


def get_official_name(user_input: str) -> str:
    """Step 1: æ™ºèƒ½åˆ«åè§£æ - å°†ç”¨æˆ·è¾“å…¥ï¼ˆå¯èƒ½æ˜¯æ˜µç§°æˆ–ä¸­æ–‡åï¼‰è½¬æ¢ä¸ºå®˜æ–¹è‹±æ–‡/æ³•æ–‡å
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥çš„é¦™æ°´åç§°ï¼ˆå¯èƒ½æ˜¯ä¸­æ–‡ã€æ˜µç§°æˆ–éƒ¨åˆ†åç§°ï¼‰
    
    Returns:
        å®˜æ–¹è‹±æ–‡/æ³•æ–‡åç§°ï¼Œå¦‚æœè½¬æ¢å¤±è´¥åˆ™è¿”å›åŸå§‹è¾“å…¥
    """
    try:
        name_prompt = f"""You are a Perfume Translator. Convert the user's input (which might be a nickname or Chinese name) into the Official English/French Name.

User Input: "{user_input}"

Return ONLY the official perfume name in English or French (e.g., "Louis Vuitton Orage", "Parfums de Marly Delina"). 
If you cannot determine the official name, return the original input unchanged.
Do not include any explanations, just the name."""

        completion = client.chat.completions.create(
            model=LLM_MODEL_ID,
            messages=[
                {"role": "system", "content": "You are a Perfume Translator. Convert perfume names to official English/French names."},
                {"role": "user", "content": name_prompt}
            ],
            temperature=0.2,
            max_tokens=100,
        )
        
        official_name = completion.choices[0].message.content.strip()
        # æ¸…ç†å¯èƒ½çš„å¼•å·æˆ–å¤šä½™å­—ç¬¦
        official_name = official_name.strip('"\'')
        
        print(f"[æ™ºèƒ½åˆ«åè§£æ] è¾“å…¥: {user_input} -> è¾“å‡º: {official_name}")
        return official_name
        
    except Exception as e:
        print(f"[æ™ºèƒ½åˆ«åè§£æ] è½¬æ¢å¤±è´¥: {str(e)}, ä½¿ç”¨åŸå§‹è¾“å…¥")
        import traceback
        traceback.print_exc()
        return user_input


@app.post("/api/analyze_scent")
async def analyze_scent(payload: ScentRequest):
    """åˆ†æé¦™æ°´æˆåˆ† - ä½¿ç”¨æ™ºèƒ½åˆ«åè§£æ + å…¨ç½‘æœç´¢ + DeepSeek æ™ºèƒ½éªŒè¯çš„ RAG æµç¨‹
    
    æµç¨‹ï¼š
    1. æ™ºèƒ½åˆ«åè§£æï¼šå°†ç”¨æˆ·è¾“å…¥è½¬æ¢ä¸ºå®˜æ–¹è‹±æ–‡/æ³•æ–‡å
    2. å…¨ç½‘æœç´¢ï¼šä½¿ç”¨å®˜æ–¹åç§°è¿›è¡Œæ— é™åˆ¶æœç´¢
    3. æ™ºèƒ½éªŒè¯ï¼šä½¿ç”¨ DeepSeek æå–æ•°æ®å¹¶ç¿»è¯‘å›ä¸­æ–‡
    4. è¿”å›ç»“æœå’Œå‚è€ƒ URL åˆ—è¡¨
    """
    if not TAVILY_AVAILABLE or not tavily_client:
        raise HTTPException(
            status_code=500,
            detail="Tavily Search is not available. Please install tavily-python and set TAVILY_API_KEY"
        )
    
    # Step 1: æ™ºèƒ½åˆ«åè§£æ (Name Standardization)
    official_name = get_official_name(payload.name)
    print(f"[Step 1] æ™ºèƒ½åˆ«åè§£æå®Œæˆ: {payload.name} -> {official_name}")
    
    # Step 2: æ··åˆå…¨ç½‘æœç´¢ (Hybrid Global Search)
    search_results = []
    search_content = ""
    reference_urls = []  # æ”¶é›†å‚è€ƒ URL
    
    try:
        # æ„é€ è‹±æ–‡æœç´¢æŸ¥è¯¢ï¼ˆå…¨ç½‘æœç´¢ï¼Œä¸é™åˆ¶ç½‘ç«™ï¼‰
        query_en = f"{official_name} perfume notes accords ingredients"
        
        print(f"[Tavilyæœç´¢] å¼€å§‹å…¨ç½‘æœç´¢: {query_en}")
        
        # è°ƒç”¨ Tavily APIï¼ˆä½¿ç”¨ advanced æ·±åº¦æœç´¢ï¼‰
        response = tavily_client.search(
            query=query_en,
            search_depth="advanced",
            max_results=8  # å¢åŠ ç»“æœæ•°é‡ä»¥æé«˜è¦†ç›–ç‡
        )
        
        print(f"[Tavilyæœç´¢] æœç´¢å®Œæˆï¼Œç»“æœæ•°é‡: {len(response.get('results', []))}")
        
        # è·å–æœç´¢ç»“æœçš„ content æ‘˜è¦
        if response.get("results"):
            for result in response.get("results", [])[:8]:
                title = result.get("title", "")
                content = result.get("content", "")
                url = result.get("url", "")
                
                if content:
                    search_results.append({
                        "title": title,
                        "content": content,
                        "url": url
                    })
                    # ç´¯ç§¯å†…å®¹æ‘˜è¦
                    search_content += f"Title: {title}\nContent: {content}\nURL: {url}\n\n"
                    # æ”¶é›†å‚è€ƒ URL
                    if url:
                        reference_urls.append(url)
        
        print(f"[Tavilyæœç´¢] æå–åˆ° {len(search_results)} æ¡æœ‰æ•ˆç»“æœ")
        print(f"[Tavilyæœç´¢] å‚è€ƒ URL æ•°é‡: {len(reference_urls)}")
    
    except Exception as e:
        # å¦‚æœæœç´¢å®Œå…¨å¤±è´¥ï¼Œä¸è¿”å›é”™è¯¯ï¼Œè€Œæ˜¯æ ‡è®°ä¸ºä½¿ç”¨å…œåº•æ–¹æ¡ˆ
        print(f"[Tavilyæœç´¢] æœç´¢é”™è¯¯: {str(e)}, å°†ä½¿ç”¨ç©ºç»“æœ")
        import traceback
        traceback.print_exc()
        search_results = []  # æ¸…ç©ºç»“æœï¼Œè§¦å‘å…œåº•é€»è¾‘
    
    # Step 3: å®½æ¾éªŒè¯ (Relaxed Validation with Translation)
    # å°† Tavily çš„æœç´¢ç»“æœå–‚ç»™ DeepSeek è¿›è¡ŒéªŒè¯å’Œæå–
    
    if not search_content or not search_results:
        # å¦‚æœæ²¡æœ‰æœç´¢ç»“æœï¼Œç›´æ¥è¿”å›æœªæ‰¾åˆ°
        print(f"[æ™ºèƒ½éªŒè¯] æ— æœç´¢ç»“æœï¼Œè¿”å› found: false")
        return {
            "found": False,
            "message": "Perfume not found in search results",
            "reference_urls": []
        }
    
    # æ„å»ºéªŒè¯æç¤ºè¯ï¼ˆæ”¯æŒè‹±æ–‡ç»“æœå¹¶ç¿»è¯‘å›ä¸­æ–‡ï¼‰
    verification_prompt = f"""You are a knowledgeable Perfume Data Analyst.

**Task:** Analyze the search snippets to identify the perfume described by the user's query: "{payload.name}" (official name: "{official_name}").

**CRITICAL RULES:**

**Language Handling:** The search results may be in English, but you must extract data and translate all descriptions back to Chinese for the final JSON output.

**Fuzzy Match:** The user's input might be a nickname, a typo, or a partial name (e.g., "è·¯æ˜“å¨ç™» é›·æš´" = "Louis Vuitton Orage").
- If the snippets discuss a perfume that clearly matches the intent (even if the name is slightly different), accept it.
- Example: User says "è·¯æ˜“å¨ç™» é›·æš´", Official name is "Louis Vuitton Orage", Snippets show "Louis Vuitton Orage". -> MATCH!
- Example: User says "ç›ä¸½ä¹‹é¦™ ç«ç‘°", Snippets show "Parfums de Marly Delina La RosÃ©e". -> MATCH!

**Correct the Name:** If found, use the official Brand & Name from the snippets in your JSON output (e.g., set "brand": "Louis Vuitton", "name": "Orage / é›·æš´").

**Extraction:** Extract all fields (radar_data, notes, etc.) based on the snippets. 
- Translate all descriptions to Chinese.
- Extract notes, accords, and ingredients from English sources.
- Convert radar data scores (0-10) based on the fragrance profile described.

**Not Found:** Only return {{"found": false}} if the snippets are completely unrelated (e.g., about a car or a politician) or "No results found".

**Search snippets:**
{search_content}

Return ONLY valid JSON, no explanations, no markdown code blocks."""
    
    system_prompt = """You are a knowledgeable Perfume Data Analyst.

**Task:** Analyze the search snippets to identify the perfume described by the user's query.

**CRITICAL RULES:**

**Language Handling:** The search results may be in English, but you must extract data and translate all descriptions back to Chinese for the final JSON output.

**Fuzzy Match:** The user's input might be a nickname, a typo, or a partial name (e.g., "è·¯æ˜“å¨ç™» é›·æš´" = "Louis Vuitton Orage").
- If the snippets discuss a perfume that clearly matches the intent (even if the name is slightly different), accept it.
- Example: User says "è·¯æ˜“å¨ç™» é›·æš´", Official name is "Louis Vuitton Orage", Snippets show "Louis Vuitton Orage". -> MATCH!

**Correct the Name:** If found, use the official Brand & Name from the snippets in your JSON output (e.g., set "brand": "Louis Vuitton", "name": "Orage / é›·æš´").

**Extraction:** Extract all fields (radar_data, notes, etc.) based on the snippets. Translate descriptions to Chinese.

**Not Found:** Only return {"found": false} if the snippets are completely unrelated (e.g., about a car or a politician) or "No results found".

**MANDATORY Output JSON Structure (when found=true):**
{
  "found": true,
  "brand": "å“ç‰Œå (bilingual format)",
  "name": "é¦™æ°´å (bilingual format: English Name / ä¸­æ–‡å)",
  "radar_data": {
    "Floral": 0-10,
    "Woody": 0-10,
    "Fresh": 0-10,
    "Spicy": 0-10,
    "Sweet": 0-10,
    "Oriental": 0-10
  },
  "notes": {
    "top": "å‰è°ƒæè¿° (ä¸­æ–‡)",
    "middle": "ä¸­è°ƒæè¿° (ä¸­æ–‡)",
    "base": "åè°ƒæè¿° (ä¸­æ–‡)"
  },
  "allergens": ["Limonene", "Linalool"],
  "longevity": "ç•™é¦™æ—¶é—´ (e.g., 'æŒä¹… 8h+')",
  "safety_brief": "å®‰å…¨ç®€è¯„ (ä¸­æ–‡)"
}

Always return valid JSON only."""
    
    try:
        completion = client.chat.completions.create(
            model=LLM_MODEL_ID,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": verification_prompt}
            ],
            temperature=0.3,
            max_tokens=1500,  # å¢åŠ  token é™åˆ¶ä»¥æ”¯æŒæ›´è¯¦ç»†çš„æ•°æ®
        )
        
        response_text = completion.choices[0].message.content.strip()
        
        # è°ƒè¯•æ—¥å¿—ï¼šæ‰“å° LLM åŸå§‹å“åº”
        print(f"[DeepSeekéªŒè¯] LLM åŸå§‹å“åº”: {response_text}")
        
        # è§£æ JSON
        if "```json" in response_text:
            response_text = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            response_text = response_text.split("```")[1].split("```")[0].strip()
        
        print(f"[DeepSeekéªŒè¯] è§£æåçš„ JSON å­—ç¬¦ä¸²: {response_text}")
        
        result = json.loads(response_text)
        
        print(f"[DeepSeekéªŒè¯] è§£æåçš„ JSON å¯¹è±¡: found={result.get('found', False)}")
        
        # æ·»åŠ æ¥æºæ ‡è®°å’Œå‚è€ƒ URL
        result["source"] = "tavily_verified"
        result["reference_urls"] = reference_urls[:5]  # æœ€å¤šè¿”å› 5 ä¸ªå‚è€ƒ URL
        
        # éªŒè¯ç»“æœ
        if not result.get("found", False):
            return {
                "found": False,
                "message": "Perfume not found in search results",
                "source": "tavily_verified",
                "reference_urls": reference_urls[:5]
            }
        
        # è¿”å›ç»“æœ
        return result
        
    except json.JSONDecodeError as e:
        print(f"[æ™ºèƒ½éªŒè¯] JSON è§£æå¤±è´¥: {str(e)}")
        return {
            "found": False,
            "message": f"Failed to parse analysis: {str(e)}",
            "source": "tavily_verified",
            "reference_urls": reference_urls[:5]
        }
    except Exception as e:
        print(f"[æ™ºèƒ½éªŒè¯] LLM è°ƒç”¨å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            "found": False,
            "message": f"Analysis failed: {str(e)}",
            "source": "tavily_verified",
            "reference_urls": reference_urls[:5]
        }


# å¯åŠ¨æœåŠ¡å™¨
if __name__ == "__main__":
    import uvicorn  # pyright: ignore[reportMissingImports]
    uvicorn.run(app, host="0.0.0.0", port=8001)
